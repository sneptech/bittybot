---
phase: 01-inference-spike
plan: 03
type: tdd
wave: 2
depends_on:
  - "01-01"
files_modified:
  - integration_test/spike_binding_load_test.dart
  - integration_test/spike_streaming_test.dart
  - integration_test/helpers/model_loader.dart
autonomous: true
requirements:
  - MODL-06

must_haves:
  truths:
    - "Tiny Aya Global Q4_K_M GGUF loads into the chosen Flutter plugin without architecture error"
    - "Tokens stream back to Dart one-at-a-time during generation, not buffered until completion"
    - "Model generates coherent text output from a simple prompt"
  artifacts:
    - path: "integration_test/spike_binding_load_test.dart"
      provides: "Integration test verifying model loads without architecture error and generates text"
      min_lines: 40
    - path: "integration_test/spike_streaming_test.dart"
      provides: "Integration test verifying tokens stream one-at-a-time with timestamp spread"
      min_lines: 40
    - path: "integration_test/helpers/model_loader.dart"
      provides: "Shared helper for resolving model path and initializing llama.cpp binding"
      min_lines: 30
  key_links:
    - from: "integration_test/spike_binding_load_test.dart"
      to: "integration_test/helpers/model_loader.dart"
      via: "imports model_loader for shared model initialization"
      pattern: "import.*model_loader"
    - from: "integration_test/helpers/model_loader.dart"
      to: "llama_cpp_dart"
      via: "creates Llama instance with GGUF path and context params"
      pattern: "Llama\\("
---

<objective>
Write integration tests (TDD red phase) that verify Tiny Aya Global Q4_K_M GGUF loads without architecture error and streams tokens one-at-a-time, then implement the minimal model loader helper to make them pass (green phase).

Purpose: This is the go/no-go gate for the entire project. If the model does not load or tokens do not stream, the binding choice must change or the project approach must be reconsidered. Tests are written first per the locked TDD decision.
Output: Two integration test files and a shared model loader helper that together verify the core binding functionality.
</objective>

<execution_context>
@/home/max/.claude/mowism/workflows/execute-plan.md
@/home/max/.claude/mowism/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/01-inference-spike/01-RESEARCH.md
@.planning/phases/01-inference-spike/01-CONTEXT.md
@.planning/phases/01-inference-spike/01-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Write integration tests for model loading and token streaming (RED phase)</name>
  <files>
    integration_test/spike_binding_load_test.dart
    integration_test/spike_streaming_test.dart
  </files>
  <action>
    **This is the RED phase of TDD. Write tests that describe the expected behavior. They will NOT pass yet because the model_loader helper does not exist.**

    1. Create `integration_test/spike_binding_load_test.dart`:

    ```dart
    import 'package:flutter_test/flutter_test.dart';
    import 'package:integration_test/integration_test.dart';
    import 'helpers/model_loader.dart';

    void main() {
      final binding = IntegrationTestWidgetsFlutterBinding.ensureInitialized();
      binding.defaultTestTimeout = Timeout.none;

      group('Phase 1 Spike: Binding Load', () {

        testWidgets('Tiny Aya Global Q4_K_M loads without architecture error', (tester) async {
          // Arrange
          final loader = ModelLoader();

          // Act — this is the critical go/no-go check
          final result = await loader.loadModel();

          // Assert
          expect(result.loaded, isTrue,
            reason: 'Model must load without architecture error. '
                    'If this fails, the llama.cpp version in the binding '
                    'does not support Cohere2 architecture.');
          expect(result.architectureError, isNull,
            reason: 'No architecture error should be reported');
          expect(result.modelInfo, isNotNull);
          expect(result.modelInfo!.contextSize, greaterThan(0));
        }, timeout: Timeout.none);

        testWidgets('model generates non-empty text from a simple English prompt', (tester) async {
          // Arrange
          final loader = ModelLoader();
          await loader.loadModel();

          // Act
          final output = await loader.generateComplete(
            'Translate "Hello" into French.',
          );

          // Assert
          expect(output, isNotEmpty,
            reason: 'Model must produce non-empty output from a simple prompt');
          expect(output.length, greaterThan(2),
            reason: 'Output should be more than just whitespace or a single character');

          // Cleanup
          loader.dispose();
        }, timeout: Timeout.none);

        testWidgets('model handles Aya chat template format', (tester) async {
          // Arrange
          final loader = ModelLoader();
          await loader.loadModel();

          // Act — use the full Aya chat template
          final output = await loader.generateComplete(
            '<|START_OF_TURN_TOKEN|><|USER_TOKEN|>Translate "Good morning" into Spanish.<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>',
          );

          // Assert
          expect(output, isNotEmpty);
          // Spanish output should contain Latin characters
          expect(RegExp(r'[a-záéíóúñü]', caseSensitive: false).hasMatch(output), isTrue,
            reason: 'Spanish translation should contain Latin characters');

          loader.dispose();
        }, timeout: Timeout.none);
      });
    }
    ```

    2. Create `integration_test/spike_streaming_test.dart`:

    ```dart
    import 'package:flutter_test/flutter_test.dart';
    import 'package:integration_test/integration_test.dart';
    import 'helpers/model_loader.dart';

    void main() {
      final binding = IntegrationTestWidgetsFlutterBinding.ensureInitialized();
      binding.defaultTestTimeout = Timeout.none;

      group('Phase 1 Spike: Token Streaming', () {

        testWidgets('tokens arrive one-at-a-time during generation (not buffered)', (tester) async {
          // Arrange
          final loader = ModelLoader();
          await loader.loadModel();

          final tokens = <String>[];
          final timestamps = <DateTime>[];

          // Act — stream tokens and record arrival timestamps
          await for (final token in loader.generateStream(
            '<|START_OF_TURN_TOKEN|><|USER_TOKEN|>Write a short paragraph about the weather.<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>',
          )) {
            tokens.add(token);
            timestamps.add(DateTime.now());
          }

          // Assert — streaming verification
          expect(tokens.length, greaterThan(5),
            reason: 'Should receive multiple individual tokens, not one big chunk');

          // Timestamps should span the generation time, not cluster at the end
          final totalDuration = timestamps.last.difference(timestamps.first);
          expect(totalDuration.inMilliseconds, greaterThan(500),
            reason: 'Token timestamps should span generation time (>500ms), '
                    'indicating true streaming. If all tokens arrive within a few ms, '
                    'the output is being buffered.');

          // Check that tokens arrive incrementally (not all at once)
          // At least 3 distinct 100ms buckets should have tokens
          final buckets = <int>{};
          for (final ts in timestamps) {
            buckets.add(ts.difference(timestamps.first).inMilliseconds ~/ 100);
          }
          expect(buckets.length, greaterThan(3),
            reason: 'Tokens should arrive across multiple time windows, '
                    'not in a single burst');

          loader.dispose();
        }, timeout: Timeout.none);

        testWidgets('streaming produces same output as complete generation', (tester) async {
          // Arrange
          final loader = ModelLoader();
          await loader.loadModel();

          const prompt = '<|START_OF_TURN_TOKEN|><|USER_TOKEN|>Say "hello world" in Japanese.<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>';

          // Act — collect streamed tokens
          final streamedTokens = <String>[];
          await for (final token in loader.generateStream(prompt)) {
            streamedTokens.add(token);
          }
          final streamedOutput = streamedTokens.join();

          // Assert
          expect(streamedOutput, isNotEmpty);
          expect(streamedTokens.length, greaterThan(1),
            reason: 'Streaming should produce multiple tokens');

          loader.dispose();
        }, timeout: Timeout.none);

        testWidgets('token generation speed is measured', (tester) async {
          // Arrange
          final loader = ModelLoader();
          await loader.loadModel();

          final stopwatch = Stopwatch()..start();
          var tokenCount = 0;

          // Act
          await for (final _ in loader.generateStream(
            '<|START_OF_TURN_TOKEN|><|USER_TOKEN|>Translate "Where is the bathroom?" into Thai.<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>',
          )) {
            tokenCount++;
          }
          stopwatch.stop();

          // Record performance metrics (informational, not a pass/fail gate)
          final tokensPerSecond = tokenCount / (stopwatch.elapsedMilliseconds / 1000.0);
          // ignore: avoid_print
          print('Performance: $tokenCount tokens in ${stopwatch.elapsedMilliseconds}ms '
                '($tokensPerSecond tokens/sec)');

          // Assert — at least some tokens were generated
          expect(tokenCount, greaterThan(0));
          // Thai script should be present
          expect(RegExp(r'[\u0E00-\u0E7F]').hasMatch(
            (await loader.generateStream(
              '<|START_OF_TURN_TOKEN|><|USER_TOKEN|>Say "hello" in Thai.<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>',
            ).toList()).join()
          ), isTrue, reason: 'Thai translation should contain Thai script characters');

          loader.dispose();
        }, timeout: Timeout.none);
      });
    }
    ```

    **Key design decisions:**
    - Tests use `ModelLoader` (to be implemented in Task 2) as the shared abstraction
    - `ModelLoader.loadModel()` returns a result object with `loaded`, `architectureError`, `modelInfo`
    - `ModelLoader.generateComplete()` returns the full string
    - `ModelLoader.generateStream()` returns `Stream<String>` of individual tokens
    - All tests use `Timeout.none` on both the binding and individual testWidgets
    - Streaming is verified by timestamp spread (not just token count)
    - Performance metrics are printed but not used as pass/fail gates

    IMPORTANT: These tests MUST fail at this point (RED phase). The model_loader.dart does not exist yet.
  </action>
  <verify>
    - Both test files exist and have correct imports
    - `dart analyze integration_test/spike_binding_load_test.dart` reports import errors (expected — model_loader.dart doesn't exist yet) but no syntax errors
    - Tests reference `IntegrationTestWidgetsFlutterBinding` and `Timeout.none`
    - All testWidgets have `timeout: Timeout.none`
  </verify>
  <done>
    Integration test files exist for model loading and token streaming. Tests describe the expected behavior: model loads without architecture error, generates text, handles Aya chat template, streams tokens one-at-a-time with timestamp spread verification, and measures performance. Tests are in RED state (will not pass because model_loader.dart does not exist yet).
  </done>
</task>

<task type="auto">
  <name>Task 2: Implement model loader helper to make tests pass (GREEN phase)</name>
  <files>
    integration_test/helpers/model_loader.dart
  </files>
  <action>
    **This is the GREEN phase of TDD. Implement the minimal code to make all tests from Task 1 pass.**

    Create `integration_test/helpers/model_loader.dart`:

    ```dart
    import 'dart:io';
    import 'package:llama_cpp_dart/llama_cpp_dart.dart';
    import 'package:path_provider/path_provider.dart';
    ```

    **ModelLoader class design:**

    ```dart
    class ModelLoadResult {
      final bool loaded;
      final String? architectureError;
      final ModelInfo? modelInfo;
      ModelLoadResult({required this.loaded, this.architectureError, this.modelInfo});
    }

    class ModelInfo {
      final int contextSize;
      final String modelPath;
      ModelInfo({required this.contextSize, required this.modelPath});
    }

    class ModelLoader {
      Llama? _llama;
      String? _modelPath;

      /// Resolves the model file path.
      /// The GGUF must be pre-placed on the device before running tests.
      /// Expected location: app documents directory / 'tiny-aya-global-q4_k_m.gguf'
      ///
      /// For the spike, the model file must be manually pushed to the device:
      /// - iOS: Use Xcode Device Manager to copy to app's Documents
      /// - Android: `adb push tiny-aya-global-q4_k_m.gguf /sdcard/Download/`
      ///   then copy to app documents on first test run
      Future<String> _resolveModelPath() async {
        if (_modelPath != null) return _modelPath!;

        final dir = await getApplicationDocumentsDirectory();
        final modelFile = File('${dir.path}/tiny-aya-global-q4_k_m.gguf');

        if (!modelFile.existsSync()) {
          // Also check Downloads directory on Android
          final downloadPath = '/sdcard/Download/tiny-aya-global-q4_k_m.gguf';
          final downloadFile = File(downloadPath);
          if (Platform.isAndroid && downloadFile.existsSync()) {
            // Copy from Downloads to app documents
            await downloadFile.copy(modelFile.path);
          } else {
            throw StateError(
              'Model file not found at ${modelFile.path}\n'
              'Please place tiny-aya-global-q4_k_m.gguf in the app documents directory.\n'
              'Android: adb push <file> /sdcard/Download/\n'
              'iOS: Use Xcode Device Manager to copy to app Documents.'
            );
          }
        }

        _modelPath = modelFile.path;
        return _modelPath!;
      }

      Future<ModelLoadResult> loadModel() async {
        try {
          final path = await _resolveModelPath();

          _llama = Llama(
            path,
            modelParams: ModelParams(),  // defaults
            contextParams: ContextParams()
              ..nCtx = 512     // minimal context for spike — reduces memory pressure
              ..nBatch = 256,
            verbose: true,     // log llama.cpp internals for debugging
          );

          return ModelLoadResult(
            loaded: true,
            modelInfo: ModelInfo(
              contextSize: 512,
              modelPath: path,
            ),
          );
        } catch (e) {
          final errorStr = e.toString();
          // Check for architecture-specific errors
          if (errorStr.contains('architecture') ||
              errorStr.contains('unknown model') ||
              errorStr.contains('not supported')) {
            return ModelLoadResult(
              loaded: false,
              architectureError: errorStr,
            );
          }
          rethrow;
        }
      }

      /// Generate complete output (waits for all tokens)
      Future<String> generateComplete(String prompt) async {
        _ensureLoaded();
        _llama!.setPrompt(prompt);
        final tokens = <String>[];
        await for (final text in _llama!.generateText()) {
          tokens.add(text);
        }
        return tokens.join();
      }

      /// Stream tokens one-at-a-time as they are generated
      Stream<String> generateStream(String prompt) async* {
        _ensureLoaded();
        _llama!.setPrompt(prompt);
        await for (final text in _llama!.generateText()) {
          yield text;
        }
      }

      void _ensureLoaded() {
        if (_llama == null) {
          throw StateError('Model not loaded. Call loadModel() first.');
        }
      }

      void dispose() {
        _llama?.dispose();
        _llama = null;
      }
    }
    ```

    **IMPORTANT NOTES:**
    - The `llama_cpp_dart` API may differ from what's shown above. Check the actual API by reading the package source. The key classes are `Llama`, `ContextParams`, `ModelParams`. The streaming method is `generateText()` which returns a `Stream<String>`.
    - Use `nCtx = 512` to minimize memory pressure on the spike (not production context size)
    - The model file must be pre-placed on the device. This is a spike — production download flow comes in Phase 2.
    - If `llama_cpp_dart`'s API does not match the test expectations, adjust EITHER the tests OR the loader to align — but preserve the test INTENT (load, generate, stream).
    - If `llama_cpp_dart` fails to load the model (architecture not supported), document the error and try `fllama` as fallback per the research binding decision framework.

    After implementing, verify the tests compile:
    ```bash
    flutter analyze integration_test/
    ```

    Note: Tests cannot actually RUN yet without a physical device and model file. The GREEN phase is confirmed when the code compiles and the test structure is correct. Actual on-device execution happens in Plan 05.

    AVOID:
    - Do NOT set nCtx to 4096+ (memory pressure on 4GB devices)
    - Do NOT use dynamic library loading on iOS
    - Do NOT create a model download mechanism here — that's Phase 2
  </action>
  <verify>
    - `flutter analyze integration_test/` reports no errors
    - `integration_test/helpers/model_loader.dart` exists with ModelLoader class
    - ModelLoader has loadModel(), generateComplete(), generateStream(), dispose() methods
    - Model path resolution checks app documents directory
    - nCtx is set to 512 (not a large value)
  </verify>
  <done>
    ModelLoader helper implements the interface expected by both test files. Code compiles cleanly. The loader resolves model path from app documents directory, creates Llama instance with minimal context size (512), and provides both complete generation and streaming generation methods. Tests are now in GREEN state (compilable, ready for on-device execution).
  </done>
</task>

</tasks>

<verification>
1. `flutter analyze integration_test/` reports no errors
2. All test files use IntegrationTestWidgetsFlutterBinding and Timeout.none
3. ModelLoader provides loadModel(), generateComplete(), generateStream(), dispose()
4. Streaming test verifies timestamp spread (not just token count)
5. Tests describe all three success criteria: architecture load, text generation, token streaming
</verification>

<success_criteria>
- Two integration test files exist verifying model load and streaming
- ModelLoader helper compiles and provides the expected interface
- Tests are ready for on-device execution (pending physical device + model file)
- TDD cycle completed: RED (tests written) then GREEN (implementation)
</success_criteria>

<output>
After completion, create `.planning/phases/01-inference-spike/01-03-SUMMARY.md`
</output>

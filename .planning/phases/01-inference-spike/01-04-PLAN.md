---
phase: 01-inference-spike
plan: 04
type: tdd
wave: 3
depends_on:
  - "01-01"
  - "01-03"
files_modified:
  - integration_test/spike_multilingual_test.dart
  - integration_test/helpers/report_writer.dart
autonomous: true
requirements:
  - MODL-06

must_haves:
  truths:
    - "Translation prompts in all 70+ Aya-supported languages produce output in the correct writing system"
    - "Priority languages (Mandarin, Cantonese, Latin American Spanish, English) produce travel-quality translations"
    - "Cantonese output is distinct from Mandarin with Cantonese-specific particles"
    - "Test results are written as JSON for consumption by LLM-as-judge tooling"
  artifacts:
    - path: "integration_test/spike_multilingual_test.dart"
      provides: "Integration test verifying translation across all 70+ languages with script validation"
      min_lines: 100
    - path: "integration_test/helpers/report_writer.dart"
      provides: "Helper to serialize test results to JSON for judge scripts"
      min_lines: 40
  key_links:
    - from: "integration_test/spike_multilingual_test.dart"
      to: "integration_test/helpers/language_corpus.dart"
      via: "imports languageCorpus for test data"
      pattern: "import.*language_corpus"
    - from: "integration_test/spike_multilingual_test.dart"
      to: "integration_test/helpers/model_loader.dart"
      via: "imports ModelLoader for inference"
      pattern: "import.*model_loader"
    - from: "integration_test/spike_multilingual_test.dart"
      to: "integration_test/helpers/report_writer.dart"
      via: "writes JSON results for judge consumption"
      pattern: "import.*report_writer"
    - from: "integration_test/helpers/report_writer.dart"
      to: "tool/lib/result_types.dart"
      via: "uses same JSON schema as judge scripts expect"
      pattern: "LanguageResult|PromptResult"
---

<objective>
Write integration tests (TDD red phase) that verify multilingual translation across all 70+ Aya-supported languages using the language corpus, with special attention to priority languages and Cantonese distinction, then implement the report writer to export results for LLM-as-judge evaluation (green phase).

Purpose: Validate that the on-device model produces plausible translations in the correct writing system across all target languages, with emphasis on travel scenarios for priority languages. Export structured results for automated coherence evaluation.
Output: A comprehensive multilingual integration test and a report writer that serializes results as JSON for the judge tooling in tool/.
</objective>

<execution_context>
@/home/max/.claude/get-shit-done/workflows/execute-plan.md
@/home/max/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/01-inference-spike/01-RESEARCH.md
@.planning/phases/01-inference-spike/01-CONTEXT.md
@.planning/phases/01-inference-spike/01-01-SUMMARY.md
@.planning/phases/01-inference-spike/01-03-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Write multilingual translation integration tests and report writer (RED + GREEN)</name>
  <files>
    integration_test/spike_multilingual_test.dart
    integration_test/helpers/report_writer.dart
  </files>
  <action>
    **Combined RED+GREEN for this task since the test and report writer are tightly coupled.**

    1. Create `integration_test/helpers/report_writer.dart`:

    A helper that collects test results and writes them to a JSON file on the device's filesystem, using the same schema that `tool/lib/result_types.dart` expects.

    ```dart
    class PromptResultData {
      final String category;
      final String sourceText;
      final String prompt;
      final String generatedOutput;
      final int tokenCount;
      final double tokensPerSecond;
      final bool scriptValidationPassed;
      final int durationMs;

      // constructor, toJson()
    }

    class LanguageResultData {
      final String languageName;
      final String languageCode;
      final String scriptFamily;
      final String priority;
      final List<PromptResultData> prompts;

      // constructor, toJson()
    }

    class ReportWriter {
      final List<LanguageResultData> results = [];

      void addLanguageResult(LanguageResultData result) {
        results.add(result);
      }

      /// Writes results to JSON file in app documents directory.
      /// Returns the file path for retrieval via adb/Xcode.
      Future<String> writeResults({String filename = 'spike_results.json'}) async {
        final dir = await getApplicationDocumentsDirectory();
        final file = File('${dir.path}/$filename');
        final json = jsonEncode(results.map((r) => r.toJson()).toList());
        await file.writeAsString(json);
        // ignore: avoid_print
        print('Results written to: ${file.path}');
        return file.path;
      }
    }
    ```

    2. Create `integration_test/spike_multilingual_test.dart`:

    **Structure:**
    - One `group` for priority languages with individual `testWidgets` per language
    - One `group` for standard languages that iterates through all remaining languages
    - Each test: sends prompt through ModelLoader.generateStream(), validates script, records results

    ```dart
    import 'package:flutter_test/flutter_test.dart';
    import 'package:integration_test/integration_test.dart';
    import 'helpers/model_loader.dart';
    import 'helpers/language_corpus.dart';
    import 'helpers/report_writer.dart';

    void main() {
      final binding = IntegrationTestWidgetsFlutterBinding.ensureInitialized();
      binding.defaultTestTimeout = Timeout.none;

      late ModelLoader loader;
      final reportWriter = ReportWriter();

      setUpAll(() async {
        loader = ModelLoader();
        await loader.loadModel();
      });

      tearDownAll(() async {
        // Write all results to JSON for judge scripts
        final path = await reportWriter.writeResults();
        print('All results saved to: $path');
        print('Retrieve with: adb pull $path ./spike_results.json');
        loader.dispose();
      });

      group('Priority Languages — Travel Phrases', () {
        for (final lang in mustHaveLanguages) {
          testWidgets('${lang.languageName} — travel phrases produce correct script output', (tester) async {
            final promptResults = <PromptResultData>[];

            for (final prompt in lang.prompts) {
              final stopwatch = Stopwatch()..start();
              final tokens = <String>[];

              await for (final token in loader.generateStream(prompt.prompt)) {
                tokens.add(token);
              }
              stopwatch.stop();

              final output = tokens.join();
              final scriptOk = lang.scriptValidator.hasMatch(output);

              promptResults.add(PromptResultData(
                category: prompt.category,
                sourceText: prompt.sourceText,
                prompt: prompt.prompt,
                generatedOutput: output,
                tokenCount: tokens.length,
                tokensPerSecond: tokens.length / (stopwatch.elapsedMilliseconds / 1000.0),
                scriptValidationPassed: scriptOk,
                durationMs: stopwatch.elapsedMilliseconds,
              ));

              // Per-prompt assertion: correct script for non-English
              if (lang.languageCode != 'en') {
                expect(scriptOk, isTrue,
                  reason: '${lang.languageName} output should contain ${lang.scriptFamily} script characters. '
                          'Got: "${output.substring(0, output.length.clamp(0, 100))}"');
              }
            }

            // Cantonese-specific check
            if (lang.languageName.contains('Cantonese')) {
              final allOutput = promptResults.map((r) => r.generatedOutput).join(' ');
              final hasCantoneseParticles = RegExp(r'[㗎囉喇嘅咁咋㖖]').hasMatch(allOutput);
              expect(hasCantoneseParticles, isTrue,
                reason: 'Cantonese output should contain Cantonese-specific particles (㗎, 囉, 喇, etc.), '
                        'not just standard Mandarin. Got: "${allOutput.substring(0, allOutput.length.clamp(0, 200))}"');
            }

            reportWriter.addLanguageResult(LanguageResultData(
              languageName: lang.languageName,
              languageCode: lang.languageCode,
              scriptFamily: lang.scriptFamily.name,
              priority: lang.priority.name,
              prompts: promptResults,
            ));
          }, timeout: Timeout.none);
        }
      });

      group('Standard Languages — Reference Sentences', () {
        for (final lang in standardLanguages) {
          testWidgets('${lang.languageName} — reference sentences produce correct script output', (tester) async {
            final promptResults = <PromptResultData>[];

            for (final prompt in lang.prompts) {
              final stopwatch = Stopwatch()..start();
              final tokens = <String>[];

              await for (final token in loader.generateStream(prompt.prompt)) {
                tokens.add(token);
              }
              stopwatch.stop();

              final output = tokens.join();
              final scriptOk = lang.scriptValidator.hasMatch(output);

              promptResults.add(PromptResultData(
                category: prompt.category,
                sourceText: prompt.sourceText,
                prompt: prompt.prompt,
                generatedOutput: output,
                tokenCount: tokens.length,
                tokensPerSecond: tokens.length / (stopwatch.elapsedMilliseconds / 1000.0),
                scriptValidationPassed: scriptOk,
                durationMs: stopwatch.elapsedMilliseconds,
              ));

              // Script validation for non-Latin-script languages
              // Latin-script languages may produce valid output that also contains English
              // So we only hard-fail on non-Latin scripts producing zero target-script chars
              if (lang.scriptFamily != ScriptFamily.latin) {
                expect(scriptOk, isTrue,
                  reason: '${lang.languageName} (${lang.scriptFamily}) output should contain '
                          'target script characters. Got: "${output.substring(0, output.length.clamp(0, 100))}"');
              }
            }

            reportWriter.addLanguageResult(LanguageResultData(
              languageName: lang.languageName,
              languageCode: lang.languageCode,
              scriptFamily: lang.scriptFamily.name,
              priority: lang.priority.name,
              prompts: promptResults,
            ));
          }, timeout: Timeout.none);
        }
      });
    }
    ```

    **Key design decisions:**
    - Model is loaded ONCE in setUpAll and shared across all tests (avoids re-loading 2.14 GB per test)
    - Results are accumulated and written ONCE in tearDownAll
    - Priority languages get individual assertions per prompt (travel phrases)
    - Standard languages get script validation only (reference sentences)
    - Cantonese has explicit particle validation (not just CJK script check)
    - Latin-script languages have relaxed validation (hard to distinguish French from English by script alone; the LLM-as-judge handles quality)
    - Non-Latin-script languages have strict script validation (Arabic must contain Arabic chars, Thai must contain Thai chars, etc.)
    - All results are captured regardless of pass/fail for the report writer
    - Report JSON uses the same schema that tool/judge_quick.dart and tool/judge_full.dart expect

    **Model reuse note:** The test uses setUpAll to load the model once. Since integration tests run in a single process on-device, the model stays in memory for all tests. This is important because loading 2.14 GB takes significant time and we don't want to do it per-test.

    AVOID:
    - Do NOT skip languages or test only a subset — the user decision requires ALL 70+ languages
    - Do NOT conflate Cantonese with Chinese Traditional
    - Do NOT hardcode expected translations — use script validation + judge scripts for quality
    - Do NOT set nCtx > 512 for the spike (memory pressure)
  </action>
  <verify>
    - `flutter analyze integration_test/spike_multilingual_test.dart` reports no errors
    - `flutter analyze integration_test/helpers/report_writer.dart` reports no errors
    - The test file references all languages from language_corpus.dart
    - Cantonese test has explicit particle validation check
    - Report writer produces valid JSON with the expected schema
  </verify>
  <done>
    Multilingual integration test covers all 70+ Aya-supported languages. Priority languages (Mandarin, Cantonese, Latin American Spanish, English) have per-prompt travel phrase assertions. Standard languages have script validation. Cantonese has explicit particle validation. Report writer exports structured JSON results compatible with the LLM-as-judge tooling in tool/. Tests are ready for on-device execution.
  </done>
</task>

</tasks>

<verification>
1. `flutter analyze integration_test/` reports no errors for all test and helper files
2. Multilingual test iterates through languageCorpus (70+ entries)
3. Cantonese test checks for particles [㗎囉喇嘅咁咋㖖]
4. Report writer produces JSON file in app documents directory
5. JSON schema matches what tool/judge_quick.dart and tool/judge_full.dart expect
</verification>

<success_criteria>
- Multilingual test covers all 70+ languages from the corpus
- Priority languages have travel phrase assertions
- Cantonese validation distinguishes from Mandarin
- Script validation confirms correct writing system for non-Latin languages
- Results export as JSON for LLM-as-judge consumption
- All test code compiles without errors
</success_criteria>

<output>
After completion, create `.planning/phases/01-inference-spike/01-04-SUMMARY.md`
</output>

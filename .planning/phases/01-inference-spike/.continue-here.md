---
phase: 01-inference-spike
task: 1
total_tasks: 3
status: in_progress
last_updated: 2026-02-25T11:45:00Z
---

<current_state>
Plan 01-05 (on-device hardware verification) is in progress. The **native library loading blocker is RESOLVED** — libmtmd.so now loads and the model is recognized (Cohere2 arch + tiny_aya tokenizer confirmed). The remaining issue is that the Llama Dart wrapper fails to load the model from the app documents directory, while direct FFI loading from /data/local/tmp/ works fine.
</current_state>

<completed_work>

- Plans 01-01 through 01-04: Complete (SUMMARYs exist)
- MOW migration: GSD -> Mowism completed (commit cd676ee)
- **libmtmd.so rebuilt with static linking** — single self-contained .so, 5MB stripped:
  - Built from llama.cpp master commit `2446419` (2026-02-25)
  - Has Cohere2 architecture support AND tiny_aya pre-tokenizer (PR #19611)
  - Build: `BUILD_SHARED_LIBS=OFF`, `GGML_OPENMP=OFF`, `GGML_OPENCL=OFF`
  - Linked with `--whole-archive` and `-static-libstdc++`
  - Only depends on system libs: liblog, libandroid, libm, libdl, libc
  - 16KB page-aligned: all LOAD segments show `align 2**14`
  - Located at: `android/app/src/main/jniLibs/arm64-v8a/libmtmd.so` (gitignored)
  - Build source at: `/tmp/llama-build/llama.cpp/` and `/tmp/llama-build/build-android-arm64/`
- **spike_debug_test.dart PASSED on Galaxy A25 (SM-A256E)**:
  - DynamicLibrary.open("libmtmd.so") succeeds
  - llama_load_model_from_file() succeeds from `/data/local/tmp/tiny-aya-global-q4_k_m.gguf`
  - Architecture: cohere2, tokenizer: tiny_aya, vocab: 262144, model: 3.35B params
  - Loaded all 36 layers + 290 tensors into 2033 MB CPU buffer
  - Load time: ~1 minute 25 seconds
  - No memory pressure crashes on Galaxy A25

</completed_work>

<remaining_work>

- **FIX: ModelLoader fails to load from app documents directory**
  - Error: `Could not load model at /data/user/0/com.bittybot.bittybot/app_flutter/tiny-aya-global-q4_k_m.gguf`
  - `llama_load_model_from_file()` returns nullptr for this path
  - Direct FFI load from `/data/local/tmp/` works fine (debug test passes)
  - Root cause hypothesis: The 2.14 GB file copy from `/data/local/tmp/` to app documents failed silently (storage space? async copy issue?) OR the app documents path has permission issues
  - **Fix options:**
    a. Skip the copy — load directly from `/data/local/tmp/` (simplest for spike)
    b. Debug the copy — check if file exists and size matches after copy
    c. Use mmap=true with app documents path (may work since it's not a shell_data_file context)
  - **Recommended: Option (a)** — modify `_resolveModelPath()` to return the first found path WITHOUT copying, since copying 2.14 GB wastes storage and time
- ModelLoader error handling fixed: removed `LlamaException` from architecture error detection (was causing false negatives)
- ModelLoader now sets `nGpuLayers = 0` and `useMemorymap = false`
- After ModelLoader fix, run `spike_binding_load_test.dart` (3 tests: arch load, text gen, chat template)
- Then run `spike_streaming_test.dart` (3 tests: timestamp streaming, consistency, perf)
- Then run `spike_multilingual_test.dart` (70 languages)
- Then retrieve spike_results.json and run judge tooling (Task 3)

</remaining_work>

<decisions_made>

- Built llama.cpp from latest master (commit 2446419, 2026-02-25) — has both Cohere2 AND tiny_aya tokenizer
- Static linking approach confirmed working: single libmtmd.so with no external deps
- llama_cpp_dart 0.2.2 FFI struct definitions match latest llama.cpp (no patching needed)
- NDK 29.0.14033849 used for cross-compile (16KB alignment confirmed)
- OpenMP disabled (GGML_OPENMP=OFF) to avoid __kmpc symbol issues
- Model verified loading on Galaxy A25 with n_gpu_layers=0 and use_mmap=false

</decisions_made>

<blockers>
- **BLOCKER**: ModelLoader copies 2.14 GB to app documents dir, but load from that path fails. Fix is straightforward: load directly from /data/local/tmp/ without copying.
</blockers>

<context>
The Galaxy A25 (SM-A256E, R5CY22C2W0Z) is connected via USB, ADB working. Model file confirmed at /data/local/tmp/tiny-aya-global-q4_k_m.gguf (2,143,977,056 bytes).

**Build commands that worked:**
```bash
# Clone
git clone --depth 1 https://github.com/ggml-org/llama.cpp.git /tmp/llama-build/llama.cpp

# Configure
cmake /tmp/llama-build/llama.cpp \
  -DCMAKE_TOOLCHAIN_FILE=/home/max/Android/Sdk/ndk/29.0.14033849/build/cmake/android.toolchain.cmake \
  -DANDROID_ABI=arm64-v8a -DANDROID_PLATFORM=android-28 -DANDROID_STL=c++_shared \
  -DCMAKE_BUILD_TYPE=Release -DBUILD_SHARED_LIBS=OFF \
  -DGGML_OPENMP=OFF -DGGML_OPENCL=OFF -DGGML_VULKAN=OFF -DGGML_METAL=OFF \
  -DLLAMA_BUILD_TESTS=OFF -DLLAMA_BUILD_EXAMPLES=OFF -DLLAMA_BUILD_SERVER=OFF

# Build
cmake --build . --parallel $(nproc) --target ggml ggml-base ggml-cpu llama mtmd

# Link into single .so
$TOOLCHAIN/bin/aarch64-linux-android28-clang++ -shared -o libmtmd.so \
  -Wl,--whole-archive libmtmd.a libllama.a libggml.a libggml-base.a libggml-cpu.a \
  -Wl,--no-whole-archive -static-libstdc++ -llog -landroid -lm -ldl

# Strip
$TOOLCHAIN/bin/llvm-strip libmtmd.so

# Copy to jniLibs
cp libmtmd.so android/app/src/main/jniLibs/arm64-v8a/
```

**Verification status across phases:**
- Phase 1: Plans 01-04 code PASS, Plan 05 in progress (Android tests running)
- Phase 2: Verification chain PASS, 5 human verification items pending (real-device testing)
- Phase 3: Stages 1-3 PASS, Stage 4 (/mow:verify-work) and Stage 5 PENDING, visual verification never done
</context>

<next_action>
1. Fix ModelLoader to load directly from /data/local/tmp/ without copying (change _resolveModelPath)
2. Re-run spike_binding_load_test.dart on Galaxy A25
3. If that passes, run spike_streaming_test.dart and spike_multilingual_test.dart
4. Retrieve spike_results.json and run judge tooling
5. Consider running Phase 2 and 3 human verification items while device is connected
</next_action>

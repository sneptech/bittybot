---
phase: 01-inference-spike
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - tool/pubspec.yaml
  - tool/judge_quick.dart
  - tool/judge_full.dart
  - tool/generate_report.dart
  - tool/lib/coherence_rubric.dart
  - tool/lib/result_types.dart
autonomous: true
requirements:
  - MODL-06

must_haves:
  truths:
    - "Quick judge script calls Claude Sonnet 4.6 for coherence checking and gracefully skips when ANTHROPIC_API_KEY is not set"
    - "Full judge script calls Gemini 3.0 Flash for comprehensive 70+ language evaluation and gracefully skips when GOOGLE_GENAI_API_KEY is not set"
    - "Report generator produces a structured report with summary scorecard at top and expanded details below"
  artifacts:
    - path: "tool/pubspec.yaml"
      provides: "Dart package dependencies for judge scripts"
      contains: "anthropic_sdk_dart"
    - path: "tool/judge_quick.dart"
      provides: "Tier-1 coherence check via Claude Sonnet 4.6"
      min_lines: 80
    - path: "tool/judge_full.dart"
      provides: "Tier-2 comprehensive evaluation via Gemini 3.0 Flash"
      min_lines: 80
    - path: "tool/generate_report.dart"
      provides: "Report generator with scorecard and expanded details"
      min_lines: 60
    - path: "tool/lib/coherence_rubric.dart"
      provides: "Shared coherence scoring rubric definition"
    - path: "tool/lib/result_types.dart"
      provides: "Shared data types for test results and judge outputs"
  key_links:
    - from: "tool/judge_quick.dart"
      to: "anthropic_sdk_dart"
      via: "AnthropicClient with env var API key"
      pattern: "ANTHROPIC_API_KEY"
    - from: "tool/judge_full.dart"
      to: "googleai_dart"
      via: "GoogleAIClient with env var API key"
      pattern: "GOOGLE_GENAI_API_KEY"
    - from: "tool/generate_report.dart"
      to: "tool/lib/result_types.dart"
      via: "reads JSON result files and outputs formatted report"
      pattern: "result_types"
---

<objective>
Build the LLM-as-judge evaluation tooling as standalone Dart CLI scripts that read on-device test results (JSON) and evaluate translation coherence using Claude Sonnet 4.6 (quick check) and Gemini 3.0 Flash (full suite), producing a structured report with summary scorecard and expanded details.

Purpose: Automate multilingual translation quality validation so the spike's go/no-go decision is data-driven, not based on manual inspection of 70+ languages. This tooling runs on the developer's machine (not on the phone) and reads results exported by the on-device integration tests.
Output: Three executable Dart scripts in `tool/` — quick judge, full judge, and report generator — plus shared types and rubric.
</objective>

<execution_context>
@/home/max/.claude/get-shit-done/workflows/execute-plan.md
@/home/max/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/01-inference-spike/01-RESEARCH.md
@.planning/phases/01-inference-spike/01-CONTEXT.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create judge Dart package with shared types, coherence rubric, and both judge scripts</name>
  <files>
    tool/pubspec.yaml
    tool/lib/result_types.dart
    tool/lib/coherence_rubric.dart
    tool/judge_quick.dart
    tool/judge_full.dart
  </files>
  <action>
    1. Create `tool/pubspec.yaml` as a standalone Dart package:
       ```yaml
       name: bittybot_judge
       description: LLM-as-judge evaluation tooling for BittyBot inference spike
       environment:
         sdk: ^3.5.0
       dependencies:
         anthropic_sdk_dart: ^0.3.1
         googleai_dart: ^3.0.0
       ```
       Run `dart pub get` in the `tool/` directory.

    2. Create `tool/lib/result_types.dart` with shared data types:
       - `class LanguageResult` — holds: languageName, languageCode, scriptFamily, priority, prompts (list of PromptResult)
       - `class PromptResult` — holds: category, sourceText, prompt, generatedOutput, tokenCount, tokensPerSecond, scriptValidationPassed (bool), durationMs
       - `class JudgeScore` — holds: languageName, scriptScore (1-5), grammarScore (1-5), coherenceScore (1-5), isCorrectLanguage (bool), notes (String)
       - `class SpikeReport` — holds: timestamp, bindingUsed, deviceInfo, totalLanguages, passedLanguages, failedLanguages, languageResults (List<LanguageResult>), judgeScores (List<JudgeScore>)
       - Include `fromJson` and `toJson` methods on all types for JSON serialization
       - The on-device tests will write `List<LanguageResult>` as JSON; the judge scripts read it

    3. Create `tool/lib/coherence_rubric.dart` with the scoring rubric:
       - Define scoring criteria as constants:
         - Script correctness (1-5): 1=wrong script entirely, 2=mixed scripts, 3=correct script but unusual characters, 4=correct script with minor issues, 5=correct script throughout
         - Grammatical plausibility (1-5): 1=word salad, 2=some structure but mostly incoherent, 3=understandable but awkward, 4=natural with minor errors, 5=fluent
         - Coherence (1-5): 1=unrelated to prompt, 2=partially related, 3=related but incomplete, 4=good translation with minor issues, 5=excellent translation
       - Define pass/fail thresholds: script >= 3 AND grammar >= 3 AND coherence >= 3
       - Export rubric as prompt text that gets injected into judge LLM calls

    4. Create `tool/judge_quick.dart` — Tier-1 quick coherence check via Claude Sonnet 4.6:
       - Read ANTHROPIC_API_KEY from env var. If absent or empty, print clear instructions and exit 0 (soft skip, not failure):
         ```
         ANTHROPIC_API_KEY not set — skipping quick coherence check.
         To enable: export ANTHROPIC_API_KEY=sk-ant-...
         ```
       - Accept command-line argument: path to JSON results file
       - Read the JSON file, deserialize to List<LanguageResult>
       - For each must-have language (Mandarin, Cantonese, Latin American Spanish, English) plus one representative from each script family:
         - Send the generated output to Claude Sonnet 4.6 with the rubric prompt
         - Parse the JSON response into JudgeScore
       - Special Cantonese check: Include in the prompt "Is this Cantonese (Yue) or Mandarin? Look for Cantonese-specific particles (㗎, 囉, 喇, 嘅, 咁, 咋)."
       - Write judge scores to a JSON file (same directory as input, with `-quick-judge` suffix)
       - Print summary to stdout: pass/fail per language checked

       Use `anthropic_sdk_dart`:
       ```dart
       final client = AnthropicClient(apiKey: apiKey);
       final response = await client.createMessage(
         request: CreateMessageRequest(
           model: const Model.model(Models.claudeSonnet4_6),
           maxTokens: 512,
           messages: [MessageParam(role: MessageRole.user, content: MessageContent.text(judgePrompt))],
         ),
       );
       ```

    5. Create `tool/judge_full.dart` — Tier-2 comprehensive evaluation via Gemini 3.0 Flash:
       - Read GOOGLE_GENAI_API_KEY from env var. If absent or empty, print clear instructions and exit 0:
         ```
         GOOGLE_GENAI_API_KEY not set — skipping full coherence check.
         To enable: export GOOGLE_GENAI_API_KEY=...
         ```
       - Accept command-line argument: path to JSON results file
       - Read the JSON file, deserialize to List<LanguageResult>
       - Batch languages into groups of 5-10 to reduce API calls (Gemini Flash handles long prompts well)
       - For each batch, send all generated outputs with the rubric prompt
       - Parse responses into JudgeScore entries
       - Write all judge scores to JSON file (with `-full-judge` suffix)
       - Print summary to stdout: pass/fail per language

       Use `googleai_dart`:
       ```dart
       final client = GoogleAIClient(apiKey: apiKey);
       final response = await client.generateContent(
         modelId: 'gemini-2.0-flash',
         request: GenerateContentRequest(
           contents: [Content(parts: [Part(text: batchPrompt)])],
         ),
       );
       ```
       Note: Use 'gemini-2.0-flash' as the model ID — this is the current stable Flash model. Verify the exact model ID string from googleai_dart docs if needed.

    AVOID:
    - Do NOT hardcode API keys anywhere
    - Do NOT use `google_generative_ai` (deprecated) or `firebase_ai` (requires Firebase)
    - Do NOT use `claude_dart_flutter` (outdated, no streaming)
    - Do NOT make judge scripts depend on Flutter — they are pure Dart CLI tools
  </action>
  <verify>
    - `cd tool && dart pub get` succeeds
    - `dart analyze tool/` reports no errors
    - `dart run tool/judge_quick.dart` with no args prints usage message (not a crash)
    - `dart run tool/judge_full.dart` with no args prints usage message (not a crash)
    - Without API keys set, both scripts exit 0 with clear instructions
  </verify>
  <done>
    Both judge scripts compile, read API keys from environment variables, and gracefully skip when keys are absent. The shared types support JSON serialization for reading on-device test results. The coherence rubric defines clear 1-5 scoring criteria with pass/fail thresholds.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create report generator with summary scorecard and expanded details</name>
  <files>
    tool/generate_report.dart
  </files>
  <action>
    Create `tool/generate_report.dart` — reads the JSON results from on-device tests and optional judge scores, produces the final structured report.

    **Input:** Command-line arguments:
    - `--results <path>`: Path to on-device test results JSON (required)
    - `--quick-judge <path>`: Path to quick judge scores JSON (optional)
    - `--full-judge <path>`: Path to full judge scores JSON (optional)
    - `--output <path>`: Path to write the report (default: stdout)
    - `--format <text|json|markdown>`: Output format (default: markdown)

    **Report structure (per user decision — summary scorecard first, expanded details second):**

    **Section 1: Summary Scorecard**
    ```
    # BittyBot Inference Spike Report
    Generated: {timestamp}
    Binding: {llama_cpp_dart|fllama}
    Device: {device info from results}

    ## Summary Scorecard

    Total languages tested: {N}
    Passed: {N} ({percentage}%)
    Failed: {N} ({percentage}%)
    Skipped: {N}

    ### Priority Languages
    | Language | Script | Grammar | Coherence | Correct Lang | Status |
    |----------|--------|---------|-----------|--------------|--------|
    | Chinese (Mandarin) | 5/5 | 4/5 | 4/5 | Yes | PASS |
    | Cantonese | 4/5 | 3/5 | 3/5 | Yes | PASS |
    | ...

    ### By Script Family
    | Script Family | Languages | Passed | Failed | Pass Rate |
    |---------------|-----------|--------|--------|-----------|
    | Latin | 41 | 40 | 1 | 97.6% |
    | Arabic (RTL) | 3 | 3 | 0 | 100% |
    | ...

    ### Quick Failures (if any)
    | Language | Issue | Notes |
    |----------|-------|-------|
    | ... | Wrong script | Generated Latin instead of Thai |
    ```

    **Section 2: Expanded Details**
    ```
    ## Expanded Details

    ### Chinese (Mandarin) — PASS
    Script: CJK | Priority: must-have
    Prompts tested: 18
    Average tokens/sec: 12.3

    #### Travel Phrases
    | Category | Source | Output | Script OK | Judge |
    |----------|--------|--------|-----------|-------|
    | directions | "Where is the nearest subway station?" | "最近的地铁站在哪里？" | Yes | 5/5 |
    | ...

    #### Judge Notes
    Quick (Sonnet 4.6): "Correct Mandarin Chinese. Natural phrasing..."
    Full (Gemini Flash): "Score 4.5/5. Minor formality..."

    ---

    ### Cantonese — PASS
    ...
    ```

    **Implementation details:**
    - Read LanguageResult list from the results JSON
    - Optionally read JudgeScore lists from quick-judge and full-judge JSONs
    - Compute aggregate statistics: per-language pass/fail, per-script-family rollup, overall totals
    - A language "passes" if: script validation passed for >= 80% of prompts AND (if judge scores exist) all judge scores >= 3
    - A language without judge scores passes based on script validation only (noted as "script-only")
    - Sort priority languages first, then alphabetically within each group
    - For markdown format, use tables and headers
    - For JSON format, output the full structured report as JSON
    - For text format, use aligned columns

    AVOID:
    - Do NOT require judge scores to generate a report — the on-device script validation results alone should produce a usable report
    - Do NOT crash on missing or malformed data — handle gracefully with warnings
  </action>
  <verify>
    - `dart analyze tool/generate_report.dart` reports no errors
    - `dart run tool/generate_report.dart --help` prints usage without crashing
    - Create a small mock JSON results file and run: `dart run tool/generate_report.dart --results mock.json --format markdown` — should produce formatted output with scorecard and details sections
  </verify>
  <done>
    Report generator reads on-device test results JSON and optional judge score JSONs, producing a structured report with: (1) summary scorecard showing per-language and per-script-family pass/fail at a glance, and (2) expanded details with sample translations, judge notes, and per-prompt breakdowns. Works without judge scores (script-validation-only mode). Supports markdown, JSON, and text output formats.
  </done>
</task>

</tasks>

<verification>
1. `cd tool && dart pub get` succeeds
2. `dart analyze tool/` reports no errors
3. Both judge scripts gracefully skip without API keys (exit 0)
4. Report generator produces valid markdown output from mock data
5. Shared types correctly serialize/deserialize JSON
</verification>

<success_criteria>
- tool/ directory contains a complete Dart package with judge_quick.dart, judge_full.dart, generate_report.dart
- Judge scripts use anthropic_sdk_dart and googleai_dart respectively
- API keys read from ANTHROPIC_API_KEY and GOOGLE_GENAI_API_KEY env vars
- Graceful skip with clear instructions when keys absent
- Report has two sections: summary scorecard (scannable) and expanded details (full per-language data)
- All Dart files analyze cleanly
</success_criteria>

<output>
After completion, create `.planning/phases/01-inference-spike/01-02-SUMMARY.md`
</output>

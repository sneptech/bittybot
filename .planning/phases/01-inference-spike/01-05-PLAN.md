---
phase: 01-inference-spike
plan: 05
type: execute
wave: 4
depends_on:
  - "01-02"
  - "01-03"
  - "01-04"
files_modified: []
autonomous: false
requirements:
  - MODL-06

must_haves:
  truths:
    - "Integration tests pass on a physical iOS device with static library linking"
    - "Integration tests pass on a physical or emulated Android device"
    - "Android .so files are 16KB page-aligned (NDK r28+)"
    - "Spike results JSON is retrieved from device and available for judge evaluation"
  artifacts: []
  key_links: []
---

<objective>
Run the complete integration test suite on physical hardware (iOS device and Android), verify platform-specific requirements (iOS static linking + Metal, Android 16KB page alignment), retrieve test results JSON for LLM-as-judge evaluation, and run the judge tooling to produce the final spike report.

Purpose: This is the final validation step — confirming the spike works on real hardware, not just in analysis. The go/no-go decision for the entire project hinges on these results.
Output: Verified hardware execution, 16KB alignment check, spike results JSON, and the final coherence report.
</objective>

<execution_context>
@/home/max/.claude/get-shit-done/workflows/execute-plan.md
@/home/max/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/01-inference-spike/01-RESEARCH.md
@.planning/phases/01-inference-spike/01-CONTEXT.md
@.planning/phases/01-inference-spike/01-03-SUMMARY.md
@.planning/phases/01-inference-spike/01-04-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Prepare model file and run integration tests on Android</name>
  <files></files>
  <action>
    **Pre-requisite:** The Tiny Aya Global Q4_K_M GGUF (~2.14 GB) must be downloaded from HuggingFace. If not already available locally:
    ```bash
    # Download from HuggingFace (one-time)
    wget https://huggingface.co/CohereLabs/tiny-aya-global-GGUF/resolve/main/tiny-aya-global-q4_k_m.gguf
    ```

    **Android execution:**

    1. Ensure an Android device or emulator is connected:
       ```bash
       adb devices
       ```

    2. Push the model file to the device:
       ```bash
       adb push tiny-aya-global-q4_k_m.gguf /sdcard/Download/
       ```

    3. Run the full integration test suite on Android:
       ```bash
       flutter test integration_test/ --timeout none
       ```
       This runs all three test files:
       - spike_binding_load_test.dart (model load + generation)
       - spike_streaming_test.dart (token streaming verification)
       - spike_multilingual_test.dart (70+ language translation)

    4. If tests pass, retrieve the results JSON:
       ```bash
       # Find the results file path from test output
       adb shell "find /data/data/com.bittybot.bittybot/files -name 'spike_results.json'"
       adb pull <path> ./spike_results_android.json
       ```

    5. Verify 16KB page alignment on the built .so files:
       ```bash
       # Build release APK first
       flutter build apk --release

       # Extract and check alignment
       unzip -o build/app/outputs/flutter-apk/app-release.apk "lib/arm64-v8a/*.so" -d /tmp/apk_check

       # Check alignment of all .so files
       # NDK r28+ should produce align 2**14 for all LOAD segments
       # Use llvm-objdump from the NDK
       NDK_PATH="$HOME/Android/Sdk/ndk/28.0.12674087"
       LLVM_OBJDUMP="$NDK_PATH/toolchains/llvm/prebuilt/linux-x86_64/bin/llvm-objdump"
       "$LLVM_OBJDUMP" -p /tmp/apk_check/lib/arm64-v8a/libllama.so | grep -A1 LOAD
       ```
       All LOAD segments must show `align 2**14` (not `2**12`).

    6. Record results:
       - Test pass/fail status for each test file
       - Token generation speed (from test output)
       - 16KB alignment verification result
       - Any errors or warnings

    **If model loading fails with architecture error:**
    This means llama_cpp_dart's bundled llama.cpp does not support Cohere2. Follow the binding decision framework from the research:
    1. Check if updating llama_cpp_dart's llama.cpp submodule to latest fixes it
    2. If not, try fllama as fallback
    3. Document the failure and which binding (if any) works

    **If tokenization produces garbage:**
    This means the Tiny Aya tokenizer PR (#19611) is not included. The binding's llama.cpp must be updated to Feb 16, 2026 or later. Document and attempt submodule update.
  </action>
  <verify>
    - All integration tests pass on Android device
    - `spike_results.json` is retrieved from device
    - LOAD segments in libllama.so show `align 2**14`
  </verify>
  <done>
    Integration tests pass on Android. Model loads without architecture error, tokens stream correctly, multilingual translations produce output in correct scripts. Android .so files are 16KB page-aligned. Results JSON retrieved for judge evaluation.
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <name>Task 2: Run integration tests on physical iOS device and verify results</name>
  <files></files>
  <action>
    **CHECKPOINT: Human verification required — iOS physical device testing.**

    What was built: Complete integration test suite for the inference spike — model loading, token streaming, and 70+ language multilingual translation tests. The tests have been verified on Android (Task 1). Now they must run on a physical iOS device to confirm static library linking, Metal GPU usage, and memory behavior with the 2.14 GB model on a real iPhone.

    Steps for the user to verify:

    1. Connect a physical iOS device (iPhone 12 or newer recommended) via USB
    2. Open the project in Xcode: `open ios/Runner.xcworkspace`
    3. Select the physical device as the build target (NOT Simulator)
    4. Verify build settings:
       - CODE_SIGN_ENTITLEMENTS points to Runner/Runner.entitlements
       - Extended Virtual Addressing is enabled
       - Metal framework is linked
       - Build architecture is arm64 only

    5. Place the model file on the iOS device:
       - In Xcode: Window > Devices and Simulators
       - Select the device, find the app, click the gear icon
       - "Download Container" or drag the GGUF file into the app's Documents folder
       - Alternatively, modify the test to download from a local network URL for first run

    6. Run integration tests from the command line:
       ```bash
       flutter test integration_test/ --timeout none -d <ios-device-id>
       ```
       Or from Xcode: select the integration_test scheme and run.

    7. Check results:
       - All tests pass (model loads, tokens stream, 70+ languages produce correct scripts)
       - No JETSAM/memory pressure crashes
       - Token generation speed is noted
       - Retrieve spike_results.json from the device

    8. If tests fail, report:
       - Which specific tests failed
       - Error messages (especially architecture errors, memory errors, Metal errors)
       - Xcode console output

    **Expected outcome:** All tests pass on iOS. If any fail, this blocks the project and requires investigation per the research binding decision framework.

    **Resume signal:** Report test results: "All tests pass on iOS" or describe failures. Include token generation speed if available.
  </action>
  <verify>
    - All integration tests pass on physical iOS device
    - No JETSAM or memory pressure crashes during model load and inference
    - spike_results.json retrieved from iOS device
  </verify>
  <done>
    Integration tests confirmed working on physical iOS device with static library linking and Metal GPU. Model loads and generates tokens without memory pressure issues. Results JSON retrieved for judge evaluation.
  </done>
</task>

<task type="auto">
  <name>Task 3: Run LLM-as-judge evaluation and generate final spike report</name>
  <files></files>
  <action>
    After retrieving spike_results.json from both devices:

    1. Run quick judge (Claude Sonnet 4.6) on the results:
       ```bash
       cd tool
       dart run judge_quick.dart ../spike_results_android.json
       ```
       This produces `spike_results_android-quick-judge.json`

    2. Run full judge (Gemini Flash) on the results:
       ```bash
       dart run judge_full.dart ../spike_results_android.json
       ```
       This produces `spike_results_android-full-judge.json`

    3. Generate the final report:
       ```bash
       dart run generate_report.dart \
         --results ../spike_results_android.json \
         --quick-judge ../spike_results_android-quick-judge.json \
         --full-judge ../spike_results_android-full-judge.json \
         --format markdown \
         --output ../spike_report.md
       ```

    4. If iOS results are also available, generate a separate iOS report or a combined report.

    5. Review the report:
       - Summary scorecard: Check overall pass rate
       - Priority languages: All four must pass (Mandarin, Cantonese, Latin American Spanish, English)
       - Script families: Check for any script family with 0% pass rate (would indicate a systematic issue)
       - Cantonese: Verify it was correctly distinguished from Mandarin in judge evaluation
       - Performance: Note tokens/sec across devices

    6. Print the go/no-go decision based on:
       - Model loads without architecture error: YES/NO
       - Tokens stream one-at-a-time: YES/NO
       - 3+ language families produce correct output: YES/NO (requires Latin + Arabic + Thai at minimum)
       - iOS physical device works: YES/NO
       - Android 16KB alignment: YES/NO

    If ALL five are YES: GO — proceed to Phase 2.
    If ANY is NO: NO-GO — document the blocker and required resolution.

    Note: If ANTHROPIC_API_KEY or GOOGLE_GENAI_API_KEY are not set, the judge scripts will soft-skip. The report will still be generated from script-validation-only data. This is acceptable — the judge evaluation is a quality enhancement, not a gate.
  </action>
  <verify>
    - spike_report.md exists with summary scorecard and expanded details
    - Go/no-go decision is clearly stated
    - All five success criteria are evaluated
  </verify>
  <done>
    Final spike report generated with summary scorecard showing per-language and per-script-family pass/fail rates. LLM-as-judge coherence scores included (if API keys available). Go/no-go decision documented for the project. Phase 1 success criteria evaluated against actual on-device results.
  </done>
</task>

</tasks>

<verification>
1. Integration tests pass on both Android and iOS physical device
2. Android .so files show 16KB page alignment (align 2**14)
3. iOS runs with static library linking and Metal (no dylib, no Simulator)
4. spike_results.json retrieved from at least one device
5. Final report contains summary scorecard + expanded details
6. Go/no-go decision clearly documented against all five success criteria
</verification>

<success_criteria>
- Model loads and generates on real hardware (both platforms)
- Tokens stream one-at-a-time (verified by timestamp spread)
- 70+ languages produce output in correct writing systems
- Priority languages produce travel-quality translations (confirmed by script validation + optional judge)
- Android 16KB page alignment verified
- iOS physical device with static linking confirmed
- Go/no-go decision documented in spike report
</success_criteria>

<output>
After completion, create `.planning/phases/01-inference-spike/01-05-SUMMARY.md`
</output>

---
phase: 04-core-inference-architecture
plan: 04
type: execute
wave: 3
depends_on: ["04-02", "04-03"]
files_modified:
  - lib/features/inference/application/llm_service_provider.dart
  - lib/features/inference/application/llm_service_provider.g.dart
  - lib/features/chat/application/chat_repository_provider.dart
  - lib/widgets/app_startup_widget.dart
  - lib/widgets/app_startup_widget.g.dart
autonomous: true
requirements:
  - MODL-05

must_haves:
  truths:
    - "modelReadyProvider spawns the LlmService and loads the model as a keepAlive async operation"
    - "appStartupProvider gates the UI behind both settings AND model readiness on first launch"
    - "On subsequent launches, model loads in background while chat history and settings are accessible"
    - "chatRepositoryProvider makes DriftChatRepository available to all notifiers via Riverpod"
  artifacts:
    - path: "lib/features/inference/application/llm_service_provider.dart"
      provides: "modelReadyProvider (keepAlive AsyncNotifier) managing LlmService lifecycle"
      contains: "class ModelReady extends _\\$ModelReady"
    - path: "lib/features/chat/application/chat_repository_provider.dart"
      provides: "chatRepositoryProvider exposing DriftChatRepository"
      contains: "chatRepository"
    - path: "lib/widgets/app_startup_widget.dart"
      provides: "Extended appStartupProvider with model readiness gate"
      contains: "modelReadyProvider"
  key_links:
    - from: "lib/features/inference/application/llm_service_provider.dart"
      to: "lib/features/inference/application/llm_service.dart"
      via: "ModelReady.build() creates and starts LlmService"
      pattern: "LlmService"
    - from: "lib/features/inference/application/llm_service_provider.dart"
      to: "lib/features/model_distribution/providers.dart"
      via: "ref.read(modelDistributionProvider.notifier).modelFilePath"
      pattern: "modelDistributionProvider"
    - from: "lib/widgets/app_startup_widget.dart"
      to: "lib/features/inference/application/llm_service_provider.dart"
      via: "appStartupProvider watches modelReadyProvider"
      pattern: "modelReadyProvider"
---

<objective>
Wire the Riverpod provider graph that connects model distribution, inference service, chat repository, and app startup. This plan creates the provider layer that ChatNotifier and TranslationNotifier will consume.

Purpose: Establishes the dependency injection graph so all consumers get the LlmService and ChatRepository via standard Riverpod patterns. Extends appStartupProvider to gate the UI behind model readiness.
Output: Three new provider files plus modified app_startup_widget.dart.
</objective>

<execution_context>
@~/.claude/mowism/workflows/execute-plan.md
@~/.claude/mowism/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/04-core-inference-architecture/04-RESEARCH.md
@.planning/phases/04-core-inference-architecture/04-02-SUMMARY.md
@.planning/phases/04-core-inference-architecture/04-03-SUMMARY.md
@lib/widgets/app_startup_widget.dart
@lib/features/model_distribution/providers.dart
@lib/features/model_distribution/model_distribution_notifier.dart
@lib/features/model_distribution/model_distribution_state.dart
@lib/core/db/app_database.dart
</context>

<tasks>

<task type="auto">
  <name>Task 1: Riverpod providers — modelReadyProvider, chatRepositoryProvider, databaseProvider</name>
  <files>
    lib/features/inference/application/llm_service_provider.dart
    lib/features/inference/application/llm_service_provider.g.dart
    lib/features/chat/application/chat_repository_provider.dart
  </files>
  <action>
**databaseProvider** (in `chat_repository_provider.dart` or a shared core provider file):
Since `AppDatabase` is already constructed in `app_database.dart`, create a simple keepAlive provider:
```dart
@Riverpod(keepAlive: true)
AppDatabase appDatabase(Ref ref) {
  final db = AppDatabase();
  ref.onDispose(() => db.close());
  return db;
}
```

**chatRepositoryProvider** (`chat_repository_provider.dart`):
```dart
@Riverpod(keepAlive: true)
ChatRepository chatRepository(Ref ref) {
  final db = ref.watch(appDatabaseProvider);
  return DriftChatRepository(db);
}
```
This creates a single DriftChatRepository backed by the app database.

**modelReadyProvider** (`llm_service_provider.dart`):
```dart
@Riverpod(keepAlive: true)
class ModelReady extends _$ModelReady {
  LlmService? _llmService;

  @override
  Future<LlmService> build() async {
    // 1. Get model file path from Phase 2's ModelDistributionNotifier
    // The model must already be downloaded and verified before this provider runs.
    final notifier = ref.read(modelDistributionProvider.notifier);
    final modelPath = notifier.modelFilePath;

    // 2. Create and start the LlmService (spawns isolate, loads model)
    _llmService = LlmService(modelPath: modelPath);
    await _llmService!.start();

    // 3. Dispose on provider teardown
    ref.onDispose(() {
      _llmService?.dispose();
    });

    return _llmService!;
  }
}
```

The provider returns `AsyncValue<LlmService>`, so consumers can:
- `ref.watch(modelReadyProvider)` to get loading/error/data state
- `ref.read(modelReadyProvider).value!` to get the LlmService once loaded

**Note on the first-launch vs subsequent-launch distinction (from locked decisions):**
- **First launch:** The model download flow (Phase 2) blocks the UI anyway via `DownloadScreen`. After download completes, `_loadModel()` in `ModelDistributionNotifier` sets `ModelReadyState`. At this point `modelReadyProvider` can grab the path and start.
- **Subsequent launches:** The model file exists on disk. `ModelDistributionNotifier.initialize()` verifies SHA-256, then calls `_proceedToLoad()` which sets `LoadingModelState` then `ModelReadyState`. The `appStartupProvider` should await model readiness here, but the CONTEXT.md locked decision says "Subsequent launches: Partial access -- user can browse chat history and settings while model loads in background. Only input field is disabled until inference ready."

**Implementation approach for the two-launch distinction:**
- `appStartupProvider` should NOT await `modelReadyProvider` for subsequent launches. Instead, it should only await `settingsProvider` (as Phase 3 left it).
- `modelReadyProvider` loads independently in the background.
- The UI (ChatNotifier, TranslationNotifier in Plan 05) checks `modelReadyProvider` state and disables input when not ready.
- **However**, for first launch: the `DownloadScreen` already gates everything. The model is downloaded and verified before `MainShell` is shown. By the time `appStartupProvider` runs on first launch, the model path is available.

**Decision:** Do NOT modify `appStartupProvider` to await model readiness. Keep it as-is (settings only). The `modelReadyProvider` runs independently as a keepAlive provider. UI components watch it to enable/disable input. This matches the locked decision for partial access on subsequent launches.

Wait — the Phase 3 comment says "Phase 4 will add: await ref.watch(modelReadyProvider.future)". But the locked decision says subsequent launches allow partial access. These conflict. Resolution: the locked decision takes priority over Phase 3's anticipatory comment. Remove the Phase 3 comment and do NOT add model readiness to appStartupProvider.

**Modify `app_startup_widget.dart`:**
- Remove the comment "// Phase 4 will add: await ref.watch(modelReadyProvider.future);"
- Add a comment explaining the design: model loads independently via modelReadyProvider, UI disables input until ready
- Do NOT add modelReadyProvider as a dependency — the partial-access requirement means the app shell is usable during model load

Run codegen after creating both provider files:
```
/home/max/Android/flutter/bin/flutter pub run build_runner build --delete-conflicting-outputs
```
  </action>
  <verify>
Run `dart analyze lib/features/inference/application/llm_service_provider.dart lib/features/chat/application/chat_repository_provider.dart lib/widgets/app_startup_widget.dart` — zero errors. Verify `modelReadyProvider` returns `AsyncValue<LlmService>`. Verify `chatRepositoryProvider` returns `ChatRepository`. Verify `appStartupProvider` does NOT depend on `modelReadyProvider`.
  </verify>
  <done>
modelReadyProvider is a keepAlive AsyncNotifier that spawns LlmService and loads the model in background. chatRepositoryProvider provides DriftChatRepository via dependency injection. appStartupProvider remains settings-only, honoring the partial-access locked decision for subsequent launches. Model readiness checked by UI consumers directly.
  </done>
</task>

</tasks>

<verification>
After task completion:
1. `dart analyze lib/features/inference/application/ lib/features/chat/application/ lib/widgets/` — zero errors
2. Generated `.g.dart` files exist for both provider files
3. modelReadyProvider is keepAlive and returns LlmService
4. chatRepositoryProvider is keepAlive and returns ChatRepository interface
5. appStartupProvider does NOT await modelReadyProvider (partial access pattern)
6. Phase 3 anticipatory comments about Phase 4 are updated
</verification>

<success_criteria>
- Provider graph establishes clean dependency: modelDistribution -> modelReady -> LlmService
- Chat repository is injectable via Riverpod
- Model loads in background independently of app startup gate
- All providers and modified files pass dart analyze with zero errors
</success_criteria>

<output>
After completion, create `.planning/phases/04-core-inference-architecture/04-04-SUMMARY.md`
</output>

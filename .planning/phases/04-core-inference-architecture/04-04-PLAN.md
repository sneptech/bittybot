---
phase: 04-core-inference-architecture
plan: 04
type: execute
wave: 3
depends_on: ["04-02", "04-03"]
files_modified:
  - lib/features/inference/application/llm_service_provider.dart
  - lib/features/inference/application/llm_service_provider.g.dart
  - lib/features/inference/data/inference_repository_impl.dart
  - lib/features/chat/application/chat_repository_provider.dart
  - lib/widgets/app_startup_widget.dart
  - lib/widgets/app_startup_widget.g.dart
autonomous: true
requirements:
  - MODL-05

must_haves:
  truths:
    - "modelReadyProvider loads the model independently in background; UI consumers watch it for input enable/disable. appStartupProvider remains settings-only."
    - "On subsequent launches, model loads in background while chat history and settings are accessible"
    - "chatRepositoryProvider makes DriftChatRepository available to all notifiers via Riverpod"
    - "inferenceRepositoryProvider wraps LlmService in the InferenceRepository interface so notifiers depend on the abstraction"
    - "modelReadyProvider monitors AppLifecycleState and reloads the model if the OS killed the isolate while backgrounded"
  artifacts:
    - path: "lib/features/inference/application/llm_service_provider.dart"
      provides: "modelReadyProvider (keepAlive AsyncNotifier) managing LlmService lifecycle"
      contains: "class ModelReady extends _\\$ModelReady"
    - path: "lib/features/inference/data/inference_repository_impl.dart"
      provides: "LlmServiceInferenceRepository implementing InferenceRepository, plus inferenceRepositoryProvider"
      contains: "class LlmServiceInferenceRepository implements InferenceRepository"
    - path: "lib/features/chat/application/chat_repository_provider.dart"
      provides: "chatRepositoryProvider exposing DriftChatRepository"
      contains: "chatRepository"
    - path: "lib/widgets/app_startup_widget.dart"
      provides: "appStartupProvider remains settings-only; Phase 4 comment removed"
      contains: "appStartupProvider"
  key_links:
    - from: "lib/features/inference/application/llm_service_provider.dart"
      to: "lib/features/inference/application/llm_service.dart"
      via: "ModelReady.build() creates and starts LlmService"
      pattern: "LlmService"
    - from: "lib/features/inference/application/llm_service_provider.dart"
      to: "lib/features/model_distribution/providers.dart"
      via: "ref.read(modelDistributionProvider.notifier).modelFilePath"
      pattern: "modelDistributionProvider"
    - from: "lib/features/inference/data/inference_repository_impl.dart"
      to: "lib/features/inference/application/llm_service_provider.dart"
      via: "inferenceRepositoryProvider reads modelReadyProvider to get LlmService"
      pattern: "modelReadyProvider"
    - from: "lib/features/inference/data/inference_repository_impl.dart"
      to: "lib/features/inference/domain/inference_repository.dart"
      via: "LlmServiceInferenceRepository implements InferenceRepository"
      pattern: "InferenceRepository"
---

<objective>
Wire the Riverpod provider graph that connects model distribution, inference service, chat repository, and app startup. This plan creates the provider layer that ChatNotifier and TranslationNotifier will consume.

Purpose: Establishes the dependency injection graph so all consumers get the LlmService and ChatRepository via standard Riverpod patterns. Extends appStartupProvider to gate the UI behind model readiness.
Output: Three new provider files plus modified app_startup_widget.dart.
</objective>

<execution_context>
@~/.claude/mowism/workflows/execute-plan.md
@~/.claude/mowism/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/04-core-inference-architecture/04-RESEARCH.md
@.planning/phases/04-core-inference-architecture/04-02-SUMMARY.md
@.planning/phases/04-core-inference-architecture/04-03-SUMMARY.md
@lib/widgets/app_startup_widget.dart
@lib/features/model_distribution/providers.dart
@lib/features/model_distribution/model_distribution_notifier.dart
@lib/features/model_distribution/model_distribution_state.dart
@lib/core/db/app_database.dart
</context>

<tasks>

<task type="auto">
  <name>Task 1: Riverpod providers — modelReadyProvider, chatRepositoryProvider, databaseProvider</name>
  <files>
    lib/features/inference/application/llm_service_provider.dart
    lib/features/inference/application/llm_service_provider.g.dart
    lib/features/chat/application/chat_repository_provider.dart
  </files>
  <action>
**databaseProvider** (in `chat_repository_provider.dart` or a shared core provider file):
Since `AppDatabase` is already constructed in `app_database.dart`, create a simple keepAlive provider:
```dart
@Riverpod(keepAlive: true)
AppDatabase appDatabase(Ref ref) {
  final db = AppDatabase();
  ref.onDispose(() => db.close());
  return db;
}
```

**chatRepositoryProvider** (`chat_repository_provider.dart`):
```dart
@Riverpod(keepAlive: true)
ChatRepository chatRepository(Ref ref) {
  final db = ref.watch(appDatabaseProvider);
  return DriftChatRepository(db);
}
```
This creates a single DriftChatRepository backed by the app database.

**modelReadyProvider** (`llm_service_provider.dart`):
```dart
@Riverpod(keepAlive: true)
class ModelReady extends _$ModelReady with WidgetsBindingObserver {
  LlmService? _llmService;

  @override
  Future<LlmService> build() async {
    // 1. Get model file path from Phase 2's ModelDistributionNotifier
    // The model must already be downloaded and verified before this provider runs.
    final notifier = ref.read(modelDistributionProvider.notifier);
    final modelPath = notifier.modelFilePath;

    // 2. Create and start the LlmService (spawns isolate, loads model)
    _llmService = LlmService(modelPath: modelPath);
    await _llmService!.start();

    // 3. Register lifecycle observer for background kill recovery
    WidgetsBinding.instance.addObserver(this);

    // 4. Dispose on provider teardown
    ref.onDispose(() {
      WidgetsBinding.instance.removeObserver(this);
      _llmService?.dispose();
    });

    return _llmService!;
  }

  /// Background kill recovery (locked decision):
  /// When the app is foregrounded after the OS killed the isolate,
  /// detect the dead isolate and reload the model. UI shows
  /// "Reloading model..." banner via the AsyncLoading state.
  @override
  void didChangeAppLifecycleState(AppLifecycleState lifecycleState) {
    if (lifecycleState == AppLifecycleState.resumed) {
      _checkAndRecoverIsolate();
    }
  }

  Future<void> _checkAndRecoverIsolate() async {
    // If the isolate was killed by the OS while backgrounded,
    // _llmService will be in a broken state (send port closed).
    // Detect by checking if the isolate is still alive.
    // If dead: set state to AsyncLoading (triggers "Reloading model..." banner),
    // dispose old service, create new one, and reload.
    if (_llmService != null && !_llmService!.isAlive) {
      state = const AsyncLoading();
      await _llmService!.dispose();
      final notifier = ref.read(modelDistributionProvider.notifier);
      final modelPath = notifier.modelFilePath;
      _llmService = LlmService(modelPath: modelPath);
      await _llmService!.start();
      state = AsyncData(_llmService!);
    }
  }
}
```

**Note on `isAlive` check:** LlmService needs a `bool get isAlive` property that checks whether the isolate's SendPort is still functional. The simplest approach: send a lightweight ping command or check if `_isolate` is still running. Add this property to LlmService in Plan 02's action — append to the public API:
- `bool get isAlive => _isolate != null && _commandPort != null`
- This is a best-effort heuristic; the isolate being killed by the OS will null out `_isolate` via the error listener, or `_commandPort.send()` will throw on next use. The `_checkAndRecoverIsolate` method should catch errors from the send and treat them as dead-isolate signals.

The provider returns `AsyncValue<LlmService>`, so consumers can:
- `ref.watch(modelReadyProvider)` to get loading/error/data/loading state
- When `AsyncLoading` after previously being `AsyncData`: UI shows "Reloading model..." banner (locked decision)
- `ref.read(modelReadyProvider).value!` to get the LlmService once loaded

**Note on the first-launch vs subsequent-launch distinction (from locked decisions):**
- **First launch:** The model download flow (Phase 2) blocks the UI anyway via `DownloadScreen`. After download completes, `_loadModel()` in `ModelDistributionNotifier` sets `ModelReadyState`. At this point `modelReadyProvider` can grab the path and start.
- **Subsequent launches:** The model file exists on disk. `ModelDistributionNotifier.initialize()` verifies SHA-256, then calls `_proceedToLoad()` which sets `LoadingModelState` then `ModelReadyState`. The `appStartupProvider` should await model readiness here, but the CONTEXT.md locked decision says "Subsequent launches: Partial access -- user can browse chat history and settings while model loads in background. Only input field is disabled until inference ready."

**Implementation approach for the two-launch distinction:**
- `appStartupProvider` should NOT await `modelReadyProvider` for subsequent launches. Instead, it should only await `settingsProvider` (as Phase 3 left it).
- `modelReadyProvider` loads independently in the background.
- The UI (ChatNotifier, TranslationNotifier in Plan 05) checks `modelReadyProvider` state and disables input when not ready.
- **However**, for first launch: the `DownloadScreen` already gates everything. The model is downloaded and verified before `MainShell` is shown. By the time `appStartupProvider` runs on first launch, the model path is available.

**Decision:** Do NOT modify `appStartupProvider` to await model readiness. Keep it as-is (settings only). The `modelReadyProvider` runs independently as a keepAlive provider. UI components watch it to enable/disable input. This matches the locked decision for partial access on subsequent launches.

Wait — the Phase 3 comment says "Phase 4 will add: await ref.watch(modelReadyProvider.future)". But the locked decision says subsequent launches allow partial access. These conflict. Resolution: the locked decision takes priority over Phase 3's anticipatory comment. Remove the Phase 3 comment and do NOT add model readiness to appStartupProvider.

**Modify `app_startup_widget.dart`:**
- Remove the comment "// Phase 4 will add: await ref.watch(modelReadyProvider.future);"
- Add a comment explaining the design: model loads independently via modelReadyProvider, UI disables input until ready
- Do NOT add modelReadyProvider as a dependency — the partial-access requirement means the app shell is usable during model load

Run codegen after creating both provider files:
```
/home/max/Android/flutter/bin/flutter pub run build_runner build --delete-conflicting-outputs
```
  </action>
  <verify>
Run `dart analyze lib/features/inference/application/llm_service_provider.dart lib/features/chat/application/chat_repository_provider.dart lib/widgets/app_startup_widget.dart` — zero errors. Verify `modelReadyProvider` returns `AsyncValue<LlmService>`. Verify `chatRepositoryProvider` returns `ChatRepository`. Verify `appStartupProvider` does NOT depend on `modelReadyProvider`.
  </verify>
  <done>
modelReadyProvider is a keepAlive AsyncNotifier that spawns LlmService and loads the model in background. chatRepositoryProvider provides DriftChatRepository via dependency injection. appStartupProvider remains settings-only, honoring the partial-access locked decision for subsequent launches. Model readiness checked by UI consumers directly.
  </done>
</task>

<task type="auto">
  <name>Task 2: InferenceRepository concrete implementation and provider</name>
  <files>lib/features/inference/data/inference_repository_impl.dart</files>
  <action>
Create `lib/features/inference/data/` directory and `inference_repository_impl.dart`.

This file provides the concrete implementation of the `InferenceRepository` interface (defined in Plan 03's `inference_repository.dart`) by delegating to `LlmService`. It also provides a Riverpod provider for dependency injection.

**LlmServiceInferenceRepository:**
```dart
class LlmServiceInferenceRepository implements InferenceRepository {
  final LlmService _llmService;

  const LlmServiceInferenceRepository(this._llmService);

  @override
  int generate({required String prompt, required int nPredict}) =>
      _llmService.generate(prompt: prompt, nPredict: nPredict);

  @override
  void stop(int requestId) => _llmService.stop(requestId);

  @override
  void clearContext() => _llmService.clearContext();

  @override
  bool get isGenerating => _llmService.isGenerating;

  @override
  Stream<InferenceResponse> get responseStream => _llmService.responseStream;
}
```

**inferenceRepositoryProvider:**
```dart
@Riverpod(keepAlive: true)
InferenceRepository inferenceRepository(Ref ref) {
  final llmService = ref.watch(modelReadyProvider).value;
  if (llmService == null) {
    throw StateError('Model not yet loaded — inferenceRepositoryProvider accessed before modelReadyProvider resolved');
  }
  return LlmServiceInferenceRepository(llmService);
}
```

This provider reads `modelReadyProvider` to obtain the `LlmService` instance and wraps it in the interface. Consumers (ChatNotifier, TranslationNotifier in Plan 05) depend on `inferenceRepositoryProvider` — they never import `LlmService` directly.

Run codegen after creating the file:
```
/home/max/Android/flutter/bin/flutter pub run build_runner build --delete-conflicting-outputs
```
  </action>
  <verify>
Run `dart analyze lib/features/inference/data/inference_repository_impl.dart` — zero errors. Verify `LlmServiceInferenceRepository` implements all 5 members of `InferenceRepository`. Verify `inferenceRepositoryProvider` returns `InferenceRepository` (the interface type, not the concrete class).
  </verify>
  <done>
LlmServiceInferenceRepository delegates all InferenceRepository methods to LlmService. inferenceRepositoryProvider is a keepAlive provider that reads modelReadyProvider and wraps the LlmService in the interface. Plan 05 notifiers will consume this provider instead of LlmService directly.
  </done>
</task>

</tasks>

<verification>
After task completion:
1. `dart analyze lib/features/inference/application/ lib/features/chat/application/ lib/widgets/` — zero errors
2. Generated `.g.dart` files exist for both provider files
3. modelReadyProvider is keepAlive and returns LlmService
4. chatRepositoryProvider is keepAlive and returns ChatRepository interface
5. appStartupProvider does NOT await modelReadyProvider (partial access pattern)
6. Phase 3 anticipatory comments about Phase 4 are updated
</verification>

<success_criteria>
- Provider graph establishes clean dependency: modelDistribution -> modelReady -> LlmService
- Chat repository is injectable via Riverpod
- Model loads in background independently of app startup gate
- All providers and modified files pass dart analyze with zero errors
</success_criteria>

<output>
After completion, create `.planning/phases/04-core-inference-architecture/04-04-SUMMARY.md`
</output>

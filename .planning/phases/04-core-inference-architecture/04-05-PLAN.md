---
phase: 04-core-inference-architecture
plan: 05
type: execute
wave: 3
depends_on: ["04-04"]
files_modified:
  - lib/features/chat/application/chat_notifier.dart
  - lib/features/chat/application/chat_notifier.g.dart
  - lib/features/translation/application/translation_notifier.dart
  - lib/features/translation/application/translation_notifier.g.dart
autonomous: true
requirements:
  - CHAT-01
  - CHAT-02
  - CHAT-04

must_haves:
  truths:
    - "ChatNotifier maintains multi-turn conversation state with message history persisted to Drift DB"
    - "ChatNotifier builds incremental Aya prompts — first message includes system prompt, follow-ups are incremental"
    - "ChatNotifier queues new requests behind active generation and auto-starts the next on completion"
    - "Stop command preserves partial output and marks the message as truncated in DB"
    - "When context is full, ChatNotifier can start a new session carrying forward the last 3 exchanges"
    - "TranslationNotifier handles single translation requests with concise output and translation-specific system prompt"
    - "Both notifiers stream tokens to state as they arrive and persist final results to DB"
  artifacts:
    - path: "lib/features/chat/application/chat_notifier.dart"
      provides: "ChatNotifier managing multi-turn chat state, request queue, stop, DB persistence"
      contains: "class ChatNotifier"
    - path: "lib/features/translation/application/translation_notifier.dart"
      provides: "TranslationNotifier managing translation requests with streaming output"
      contains: "class TranslationNotifier"
  key_links:
    - from: "lib/features/chat/application/chat_notifier.dart"
      to: "lib/features/inference/data/inference_repository_impl.dart"
      via: "ref.read(inferenceRepositoryProvider) to get InferenceRepository"
      pattern: "inferenceRepositoryProvider"
    - from: "lib/features/chat/application/chat_notifier.dart"
      to: "lib/features/chat/data/chat_repository.dart"
      via: "ref.read(chatRepositoryProvider) to persist messages"
      pattern: "chatRepositoryProvider"
    - from: "lib/features/chat/application/chat_notifier.dart"
      to: "lib/features/inference/domain/prompt_builder.dart"
      via: "PromptBuilder.buildInitialPrompt and buildFollowUpPrompt"
      pattern: "PromptBuilder"
    - from: "lib/features/translation/application/translation_notifier.dart"
      to: "lib/features/inference/data/inference_repository_impl.dart"
      via: "ref.read(inferenceRepositoryProvider) to get InferenceRepository"
      pattern: "inferenceRepositoryProvider"
    - from: "lib/features/translation/application/translation_notifier.dart"
      to: "lib/features/inference/domain/prompt_builder.dart"
      via: "PromptBuilder.buildTranslationPrompt"
      pattern: "PromptBuilder"
---

<objective>
Build the ChatNotifier and TranslationNotifier — the Riverpod state managers that UI phases wire into. These notifiers orchestrate prompt building, inference requests, token streaming, request queueing, stop handling, and DB persistence.

Purpose: Phases 5 (Translation UI) and 6 (Chat UI) consume these notifiers directly. They are the only interface between the UI layer and the inference pipeline.
Output: Two notifier files with generated code.
</objective>

<execution_context>
@~/.claude/mowism/workflows/execute-plan.md
@~/.claude/mowism/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/04-core-inference-architecture/04-RESEARCH.md
@.planning/phases/04-core-inference-architecture/04-04-SUMMARY.md
@lib/features/inference/domain/prompt_builder.dart
@lib/features/inference/domain/inference_message.dart
@lib/features/inference/domain/inference_repository.dart
@lib/features/inference/data/inference_repository_impl.dart
@lib/features/inference/application/llm_service_provider.dart
@lib/features/chat/data/chat_repository.dart
@lib/features/chat/application/chat_repository_provider.dart
@lib/features/chat/domain/chat_message.dart
@lib/features/chat/domain/chat_session.dart
</context>

<tasks>

<task type="auto">
  <name>Task 1: ChatNotifier — multi-turn conversation state manager</name>
  <files>lib/features/chat/application/chat_notifier.dart, lib/features/chat/application/chat_notifier.g.dart</files>
  <action>
Create `lib/features/chat/application/chat_notifier.dart`.

**State class** (define in same file or a separate chat_state.dart — keep in same file for simplicity):
```dart
@immutable
class ChatState {
  final ChatSession? activeSession;
  final List<ChatMessage> messages;
  final String currentResponse;    // accumulated tokens for in-progress generation
  final bool isGenerating;
  final bool isModelReady;
  final bool isContextFull;        // true when context approaches ~90% of nCtx — UI shows "Start new session" prompt
  final int? activeRequestId;      // tracks current generation for stop matching

  const ChatState({
    this.activeSession,
    this.messages = const [],
    this.currentResponse = '',
    this.isGenerating = false,
    this.isModelReady = false,
    this.isContextFull = false,
    this.activeRequestId,
  });

  ChatState copyWith({...}); // all fields
}
```

**ChatNotifier class** — use `@riverpod` (NOT keepAlive — each screen instance gets its own notifier lifecycle):
```dart
@riverpod
class ChatNotifier extends _$ChatNotifier {
```

Wait — actually ChatNotifier should be keepAlive because chat state persists across navigation. Let me reconsider: the locked decision says "Subsequent launches: partial access — user can browse chat history." This means ChatNotifier state should survive navigation. But `keepAlive` might be too aggressive. Use `@Riverpod(keepAlive: true)` to match the pattern established by settingsProvider and modelReadyProvider.

Actually, for a chat app, each time the user opens a chat session, a new state is loaded from DB. The notifier doesn't need to be keepAlive — it can be recreated and reload from DB. Use `@riverpod` (auto-dispose) for ChatNotifier. The DB is the source of truth, not in-memory state.

Use `@riverpod` (auto-dispose):
```dart
@riverpod
class ChatNotifier extends _$ChatNotifier {
  final Queue<String> _pendingQueue = Queue<String>();
  StreamSubscription<InferenceResponse>? _responseSubscription;
  int _turnCount = 0; // tracks how many user messages have been sent in this session

  @override
  ChatState build() {
    // Check model readiness — watch modelReadyProvider for loading state
    final modelAsync = ref.watch(modelReadyProvider);
    final isModelReady = modelAsync.hasValue;

    ref.onDispose(() {
      _responseSubscription?.cancel();
    });

    return ChatState(isModelReady: isModelReady);
  }
```

**Public methods:**

`Future<void> loadSession(int sessionId)`:
- Get session from `ref.read(chatRepositoryProvider)`
- Get messages from repository
- Update state with loaded session and messages
- Clear LlmService context and replay history? NO — context replay from DB is expensive. Instead, start fresh context but show history from DB. The KV cache only has context from messages sent THIS app session. This is acceptable — after app restart, the model won't have prior context, but the user sees their history.
- Actually, for proper context: after loading a session, we need to replay the history into the KV cache via `setPrompt()` calls. But this means tokenizing ALL prior messages which could overflow nCtx=2048. Better approach: load session from DB for display, but start with fresh KV cache. If the user sends a new message, build an initial prompt with the system prompt + the new message. Prior messages are displayed but not in model context. This is the pragmatic choice for a 2048-token context window.
- Set `_turnCount = 0` (fresh context for this session load)

`Future<void> startNewSession()`:
- Create new session in DB via `chatRepositoryProvider`
- Clear LlmService context (`clearContext()`)
- Update state with new empty session
- Set `_turnCount = 0`

`Future<void> sendMessage(String text)`:
- If `state.isGenerating`: add to `_pendingQueue` and return (queuing per locked decision)
- Call `_processMessage(text)`

`Future<void> stopGeneration()`:
- If not generating, return
- Get InferenceRepository via `ref.read(inferenceRepositoryProvider)`, call `stop(state.activeRequestId!)`
- The response listener will receive `DoneResponse(stopped: true)` and handle truncation

`Future<void> startNewSessionWithCarryForward()`:
- Per locked decision: "Context full: prompt user to start a new session. Offer to carry forward the last 3 exchanges."
- Get the last 3 user-assistant exchange pairs from `state.messages` (up to 6 messages: 3 user + 3 assistant)
- Create a new session via `chatRepositoryProvider.createSession(mode: 'chat')`
- Clear LlmService context via `inferenceRepo.clearContext()`
- Insert the carried-forward messages into the new session in DB
- Build an initial prompt that includes the system prompt + carried-forward context as a summary preamble inside the first user turn (to seed the KV cache with the carried-forward context)
- Update state: new session, carried-forward messages, `_turnCount = 0`, `isContextFull = false`
- This method is called by the UI when the user accepts the "Start new session" prompt shown when `isContextFull == true`

`Future<void> _processMessage(String text)`:
1. Insert user message into DB: `chatRepository.insertMessage(sessionId: session.id, role: 'user', content: text)`
2. Update state: add user message to messages list, set `isGenerating = true`, clear `currentResponse`
3. Build prompt:
   - If `_turnCount == 0`: `PromptBuilder.buildInitialPrompt(systemPrompt: PromptBuilder.chatSystemPrompt, userMessage: text)`
   - Else: `PromptBuilder.buildFollowUpPrompt(text)`
4. Get InferenceRepository: `ref.read(inferenceRepositoryProvider)`
5. Subscribe to `inferenceRepo.responseStream` (if not already subscribed)
6. Call `inferenceRepo.generate(prompt: prompt, nPredict: 512)` — 512 tokens for chat per locked decision
7. Store the returned `requestId` in state
8. Increment `_turnCount`

**Response stream listener (`_setupResponseListener`):**
Call this once during the first `sendMessage`. Listen on `llmService.responseStream`:

`TokenResponse`:
- If `response.requestId != state.activeRequestId`, ignore (stale response)
- Append `response.token` to `state.currentResponse`
- Update state with new `currentResponse`

`DoneResponse`:
- If stopped: mark the accumulated response as truncated
- Insert assistant message into DB: `chatRepository.insertMessage(sessionId: session.id, role: 'assistant', content: state.currentResponse, isTruncated: response.stopped)`
- Auto-derive session title from first message if title is null: `chatRepository.updateSessionTitle(session.id, messages.first.content.substring(0, min(50, messages.first.content.length)))`
- Update state: add assistant message to messages, clear `currentResponse`, set `isGenerating = false`
- Check `_pendingQueue`: if not empty, dequeue and call `_processMessage()`

`ErrorResponse`:
- Insert partial response if any accumulated tokens
- Update state: set `isGenerating = false`, surface error (could add error field to ChatState)
- Check pending queue

**Context-full detection (per locked decision):**
Before each `_processMessage`, estimate token usage with `PromptBuilder.estimateTokenCount()` on the accumulated prompt text. If approaching ~90% of nCtx (2048), set `isContextFull = true` in state via `copyWith(isContextFull: true)`. The UI (Phase 6) will use this to show a "Start new session" prompt. Do NOT auto-start a new session — just signal the state.

Run codegen after creating the file:
```
/home/max/Android/flutter/bin/flutter pub run build_runner build --delete-conflicting-outputs
```
  </action>
  <verify>
Run `dart analyze lib/features/chat/application/chat_notifier.dart` — zero errors. Verify the notifier has `sendMessage`, `stopGeneration`, `loadSession`, `startNewSession` methods. Verify request queue logic is present. Verify prompt building uses PromptBuilder with different paths for first vs follow-up messages.
  </verify>
  <done>
ChatNotifier manages multi-turn conversation state. First message uses buildInitialPrompt with chat system prompt; follow-ups use buildFollowUpPrompt (incremental, KV cache appending). Request queue auto-starts next message on completion. Stop preserves partial output marked as truncated. Messages persisted to Drift DB. Context-full detection signals UI when approaching nCtx limit.
  </done>
</task>

<task type="auto">
  <name>Task 2: TranslationNotifier — translation request state manager</name>
  <files>lib/features/translation/application/translation_notifier.dart, lib/features/translation/application/translation_notifier.g.dart</files>
  <action>
Create `lib/features/translation/application/` directory and `translation_notifier.dart`.

**State class:**
```dart
@immutable
class TranslationState {
  final String sourceText;
  final String translatedText;    // accumulated tokens
  final String sourceLanguage;    // language name, e.g. "English"
  final String targetLanguage;    // language name, e.g. "French"
  final bool isTranslating;
  final bool isModelReady;
  final bool isContextFull;       // true when context approaches ~90% of nCtx — UI shows "Start new session" prompt
  final int? activeRequestId;
  final ChatSession? activeSession;  // translation session for context accumulation
  final int turnCount;             // tracks translations in this session

  const TranslationState({
    this.sourceText = '',
    this.translatedText = '',
    this.sourceLanguage = 'English',
    this.targetLanguage = 'Spanish',
    this.isTranslating = false,
    this.isModelReady = false,
    this.isContextFull = false,
    this.activeRequestId,
    this.activeSession,
    this.turnCount = 0,
  });

  TranslationState copyWith({...}); // all fields
}
```

**TranslationNotifier** — use `@Riverpod(keepAlive: true)` because translation state (last-used language pair) should persist across navigation (TRNS-05 requirement):
```dart
@Riverpod(keepAlive: true)
class TranslationNotifier extends _$TranslationNotifier {
  StreamSubscription<InferenceResponse>? _responseSubscription;

  @override
  TranslationState build() {
    final modelAsync = ref.watch(modelReadyProvider);
    final isModelReady = modelAsync.hasValue;

    ref.onDispose(() {
      _responseSubscription?.cancel();
    });

    return TranslationState(isModelReady: isModelReady);
  }
```

**Public methods:**

`void setSourceLanguage(String language)`:
- Update state with new sourceLanguage
- If language pair changed, start a new translation session (new context for terminology consistency per locked decision)

`void setTargetLanguage(String language)`:
- Update state with new targetLanguage
- Same session reset as above

`void swapLanguages()`:
- Swap sourceLanguage and targetLanguage in state
- Start new session (context no longer consistent after swap)

`Future<void> translate(String text)`:
- If model not ready, return (UI should prevent this)
- If already translating, queue? Per locked decision: "New requests queue behind active generation." Use same queue pattern as ChatNotifier.
- Update state: `sourceText = text`, clear `translatedText`, set `isTranslating = true`
1. Ensure active translation session exists: if null, create one via `chatRepositoryProvider.createSession(mode: 'translation')`
2. Insert user message into DB
3. Build prompt:
   - If `turnCount == 0`: `PromptBuilder.buildTranslationPrompt(text: text, targetLanguage: state.targetLanguage)` (which uses buildInitialPrompt with translation system prompt)
   - Else: `PromptBuilder.buildFollowUpTranslationPrompt(text: text, targetLanguage: state.targetLanguage)`
4. Get InferenceRepository via `ref.read(inferenceRepositoryProvider)`, call `generate(prompt: prompt, nPredict: 128)` — 128 tokens for translation per locked decision
5. Listen for token/done/error responses (same pattern as ChatNotifier)
6. On done: persist assistant message to DB, update state

`void stopTranslation()`:
- Same pattern as ChatNotifier.stopGeneration()
- Mark message as truncated

**Session management:**
- Translations within the same language pair accumulate in the same session (context accumulation per locked decision: "Translation context: Accumulates within session. Enables terminology consistency.")
- Changing language pair starts a new session and clears context
- Context-full detection same as ChatNotifier — signal UI when approaching nCtx limit

Run codegen:
```
/home/max/Android/flutter/bin/flutter pub run build_runner build --delete-conflicting-outputs
```
  </action>
  <verify>
Run `dart analyze lib/features/translation/application/translation_notifier.dart` — zero errors. Verify `translate`, `stopTranslation`, `setSourceLanguage`, `setTargetLanguage`, `swapLanguages` methods exist. Verify nPredict is 128 (not 512). Verify translation session accumulates context within the same language pair.
  </verify>
  <done>
TranslationNotifier handles translation requests with 128-token nPredict limit. Language pair changes start fresh sessions. Translations within the same pair accumulate context for terminology consistency. Tokens stream to state and final results persist to Drift DB. Stop preserves partial output.
  </done>
</task>

</tasks>

<verification>
After both tasks:
1. `dart analyze lib/features/chat/application/ lib/features/translation/application/` — zero errors
2. Generated `.g.dart` files exist for both notifiers
3. ChatNotifier uses nPredict=512, TranslationNotifier uses nPredict=128 (per locked decisions)
4. ChatNotifier has request queue with auto-dequeue on completion
5. Both notifiers persist messages to Drift DB
6. Both notifiers use PromptBuilder for Aya template formatting
7. Both notifiers check modelReadyProvider state before generating
8. Stop preserves partial output and marks isTruncated=true in DB
</verification>

<success_criteria>
- ChatNotifier supports multi-turn conversation with incremental prompt building
- TranslationNotifier supports translation with context accumulation per language pair
- Both notifiers stream tokens to state for real-time UI updates
- Request queue prevents concurrent generations (queues behind active)
- All messages persist to Drift DB and survive app restart
- All files pass dart analyze with zero errors
</success_criteria>

<output>
After completion, create `.planning/phases/04-core-inference-architecture/04-05-SUMMARY.md`
</output>

---
phase: 04-core-inference-architecture
plan: 02
type: execute
wave: 2
depends_on: ["04-01"]
files_modified:
  - lib/features/inference/application/inference_isolate.dart
  - lib/features/inference/application/llm_service.dart
autonomous: true
requirements:
  - MODL-05

must_haves:
  truths:
    - "Inference runs in a background isolate that does not block the Flutter main thread"
    - "The isolate loads the model once and persists for the session — no per-request respawning"
    - "Tokens stream back to the main isolate one at a time during generation"
    - "Stop command cooperatively halts generation and preserves partial output"
    - "Crash circuit breaker stops auto-restart after 3 consecutive failures"
  artifacts:
    - path: "lib/features/inference/application/inference_isolate.dart"
      provides: "Top-level isolate entry point that owns the Llama instance"
      contains: "void inferenceIsolateMain"
    - path: "lib/features/inference/application/llm_service.dart"
      provides: "LlmService class managing isolate lifecycle, spawn, communication, crash recovery"
      contains: "class LlmService"
  key_links:
    - from: "lib/features/inference/application/llm_service.dart"
      to: "lib/features/inference/application/inference_isolate.dart"
      via: "Isolate.spawn with inferenceIsolateMain entry point"
      pattern: "Isolate\\.spawn.*inferenceIsolateMain"
    - from: "lib/features/inference/application/inference_isolate.dart"
      to: "package:llama_cpp_dart"
      via: "Llama constructor and generateText() stream"
      pattern: "Llama\\("
    - from: "lib/features/inference/application/llm_service.dart"
      to: "lib/features/inference/domain/inference_message.dart"
      via: "SendPort.send with InferenceCommand/InferenceResponse types"
      pattern: "InferenceCommand|InferenceResponse"
---

<objective>
Build the inference isolate and the LlmService that manages its lifecycle. This is the core of Phase 4 — the bridge between Flutter UI and llama.cpp.

Purpose: Isolate moves inference off the main thread (solving the Phase 1 ANR issue). LlmService provides a clean Dart API for the rest of the app to send requests and receive streaming tokens.
Output: Two files — the isolate entry point and the service class.
</objective>

<execution_context>
@~/.claude/mowism/workflows/execute-plan.md
@~/.claude/mowism/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/04-core-inference-architecture/04-RESEARCH.md
@.planning/phases/04-core-inference-architecture/04-01-SUMMARY.md
@integration_test/helpers/model_loader.dart
@lib/features/inference/domain/inference_message.dart
</context>

<tasks>

<task type="auto">
  <name>Task 1: Inference isolate entry point — long-lived worker with Llama</name>
  <files>lib/features/inference/application/inference_isolate.dart</files>
  <action>
Create `inference_isolate.dart` in `lib/features/inference/application/`.

This file contains a **top-level function** (required for `Isolate.spawn`) that runs in the worker isolate. It owns the `Llama` FFI instance for the entire app session.

**Entry point signature:**
```dart
void inferenceIsolateMain(SendPort mainSendPort)
```

**Startup sequence:**
1. Create a `ReceivePort` for receiving commands from the main isolate
2. Send the `ReceivePort.sendPort` back to the main isolate via `mainSendPort.send(receivePort.sendPort)`
3. Listen on the `ReceivePort` for `InferenceCommand` messages

**Command handling (inside `receivePort.listen`):**

`LoadModelCommand`:
- Create `ModelParams` with: `nGpuLayers = 0`, `mainGpu = -1`, `useMemorymap = false` (all confirmed from Phase 1 spike — SELinux blocks mmap, no GPU backend)
- Create `ContextParams` with: `nCtx` from command, `nBatch` from command, `nUbatch` = same as nBatch
- Construct `Llama(command.modelPath, modelParams: modelParams, contextParams: contextParams, verbose: false)` — verbose=false for production (was true in spike for debugging)
- On success: send `ModelReadyResponse()` to main
- On catch: send `ErrorResponse(requestId: -1, message: e.toString())`

`GenerateCommand`:
- Set a local `bool _stopped = false`
- Call `llama!.setPrompt(command.prompt)` — this APPENDS to KV cache (do NOT call clear() first, unless ClearContextCommand was sent)
- `await for (final token in llama!.generateText())`: for each token, check `_stopped` flag. If stopped, break. Otherwise send `TokenResponse(requestId: command.requestId, token: token)` to main.
- After loop: send `DoneResponse(requestId: command.requestId, stopped: _stopped)`
- On catch: send `ErrorResponse(requestId: command.requestId, message: e.toString())`
- **nPredict handling:** Set `contextParams.nPredict = command.nPredict` before `setPrompt()`. If `ContextParams` doesn't have a runtime setter, use `SamplerParams` or control token count manually by counting tokens in the `await for` loop and breaking when count >= `nPredict`.

`StopCommand`:
- Set `_stopped = true`. The generate loop checks this between tokens and breaks cooperatively.
- **IMPORTANT:** The `_stopped` flag must be accessible from both the `GenerateCommand` handler and the `StopCommand` handler. Since `receivePort.listen` processes messages sequentially on the isolate's event loop, and `generateText()` yields control via `await for`, the stop command CAN arrive between token yields. Store `_stopped` as a variable in the closure scope of the `listen` callback, NOT inside the `GenerateCommand` handler's local scope.

`ClearContextCommand`:
- Call `llama!.clear()`. This wipes the KV cache — used when starting a new session.
- No response needed (fire-and-forget).

`ShutdownCommand`:
- Call `llama?.dispose()` to release FFI resources
- Call `receivePort.close()` to end the listener
- The isolate will terminate naturally after the port closes

**Important anti-patterns to avoid:**
- Do NOT use `llama!.clear()` between turns of the same session — this causes amnesia
- Do NOT call `Isolate.exit()` — let the port close naturally
- Do NOT import any Flutter plugins (`path_provider`, etc.) — they require the main isolate's platform channel binding
- The `Llama` object must NEVER leave this isolate — it's an FFI pointer
  </action>
  <verify>
Run `dart analyze lib/features/inference/application/inference_isolate.dart` — zero errors. Verify the file imports `llama_cpp_dart` and `inference_message.dart` only. Verify the entry point is a top-level function (not a static method or class method).
  </verify>
  <done>
inference_isolate.dart is a top-level function that handles all 5 InferenceCommand types, owns the Llama FFI instance, streams tokens via SendPort, and supports cooperative stop via a shared `_stopped` flag.
  </done>
</task>

<task type="auto">
  <name>Task 2: LlmService — isolate lifecycle manager with crash recovery</name>
  <files>lib/features/inference/application/llm_service.dart</files>
  <action>
Create `llm_service.dart` in `lib/features/inference/application/`.

`LlmService` is a plain Dart class (NOT a Riverpod provider — the provider wrapper is in Plan 04). It manages the isolate lifecycle and provides a clean API for sending commands and receiving responses.

**Constructor:**
```dart
LlmService({required String modelPath})
```
Stores `_modelPath` for use during spawn/restart.

**State:**
- `Isolate? _isolate` — the running worker isolate
- `SendPort? _commandPort` — for sending commands to the worker
- `ReceivePort? _responsePort` — for receiving responses from the worker
- `StreamController<InferenceResponse> _responseController` — broadcast stream for consumers
- `int _nextRequestId = 0` — monotonically increasing request ID
- `int _consecutiveCrashCount = 0` — crash circuit breaker counter
- `bool _isGenerating = false` — tracks whether a generation is in progress
- `static const int _maxAutoRetries = 3` — circuit breaker threshold

**Public API:**

`Future<void> start()`:
1. Create `_responsePort = ReceivePort()`
2. Create `_responseController = StreamController<InferenceResponse>.broadcast()`
3. Spawn the isolate: `_isolate = await Isolate.spawn(inferenceIsolateMain, _responsePort!.sendPort)`
4. Wait for the worker's `SendPort`: `_commandPort = await _responsePort!.first as SendPort`
5. Listen on `_responsePort` — forward all `InferenceResponse` messages to `_responseController`
6. Handle isolate errors by listening on `_isolate!.addErrorListener(...)` — on error, call `_handleCrash()`
7. Send `LoadModelCommand(modelPath: _modelPath)` to the worker
8. Wait for `ModelReadyResponse` on the response stream
9. Reset `_consecutiveCrashCount = 0` on successful load

`Stream<InferenceResponse> get responseStream`:
- Returns `_responseController.stream`

`int generate({required String prompt, required int nPredict})`:
- Assigns `_nextRequestId++`, creates `GenerateCommand`, sends via `_commandPort!.send(command)`
- Sets `_isGenerating = true`
- Returns the `requestId` (callers use this to match responses)

`void stop(int requestId)`:
- Sends `StopCommand(requestId: requestId)` via `_commandPort`

`void clearContext()`:
- Sends `ClearContextCommand()` via `_commandPort`

`bool get isGenerating => _isGenerating`:
- Updated to `false` when `DoneResponse` or `ErrorResponse` is received for the active request

`bool get isAlive => _isolate != null && _commandPort != null`:
- Best-effort check for whether the isolate is still running. Used by modelReadyProvider (Plan 04) to detect background kill by the OS and trigger model reload.

`Future<void> dispose()`:
- Send `ShutdownCommand()` to the worker
- Kill isolate: `_isolate?.kill(priority: Isolate.immediate)`
- Close `_responsePort`
- Close `_responseController`

**Crash handling (`_handleCrash()`):**
- Increment `_consecutiveCrashCount`
- If `<= _maxAutoRetries`: call `dispose()` then `start()` to respawn
- If `> _maxAutoRetries`: emit `ErrorResponse(requestId: -1, message: 'Model crashed repeatedly. Please restart the app.')` to `_responseController` and do NOT respawn
- Reset `_isGenerating = false`

**Response forwarding in the listen callback:**
- When receiving `DoneResponse` or `ErrorResponse`: set `_isGenerating = false`, reset `_consecutiveCrashCount = 0` on success (DoneResponse)
- Forward all responses to `_responseController.add(response)`

**Important:** The `_responsePort!.listen` callback must handle the initial `SendPort` message (first message from worker) separately from subsequent `InferenceResponse` messages. Use a two-phase approach:
1. First message: `_commandPort = await _responsePort!.first as SendPort`
2. Remaining messages: `_responsePort!.listen((msg) { if (msg is InferenceResponse) _responseController.add(msg); })`

Actually, since `_responsePort!.first` consumes the first element, subsequent `.listen` won't see it. But `.first` closes the stream subscription. Instead, use a `Completer<SendPort>` and a single `.listen`:

```dart
final sendPortCompleter = Completer<SendPort>();
_responsePort!.listen((message) {
  if (message is SendPort) {
    sendPortCompleter.complete(message);
  } else if (message is InferenceResponse) {
    // handle response forwarding
  }
});
_commandPort = await sendPortCompleter.future;
```
  </action>
  <verify>
Run `dart analyze lib/features/inference/application/llm_service.dart` — zero errors. Verify the class has `start()`, `generate()`, `stop()`, `clearContext()`, `dispose()` methods. Verify crash counter logic is present with threshold of 3.
  </verify>
  <done>
LlmService spawns and manages the inference isolate lifecycle. It provides a clean generate/stop/clearContext API, streams InferenceResponse to consumers, and implements crash circuit breaker with 3-retry threshold before surfacing error.
  </done>
</task>

</tasks>

<verification>
After both tasks:
1. `dart analyze lib/features/inference/application/` — zero errors
2. `inference_isolate.dart` imports `llama_cpp_dart` and handles all 5 command types
3. `llm_service.dart` spawns the isolate, sends LoadModelCommand, waits for ModelReadyResponse
4. Crash circuit breaker stops after 3 failures and emits error to response stream
5. `_isGenerating` flag tracks active generation state
</verification>

<success_criteria>
- Inference isolate entry point is a top-level function compatible with Isolate.spawn
- LlmService manages the full isolate lifecycle (start, generate, stop, crash recovery, dispose)
- Token streaming flows from Llama.generateText() through SendPort to LlmService.responseStream
- Crash circuit breaker prevents infinite crash loops
- All files pass dart analyze with zero errors
</success_criteria>

<output>
After completion, create `.planning/phases/04-core-inference-architecture/04-02-SUMMARY.md`
</output>

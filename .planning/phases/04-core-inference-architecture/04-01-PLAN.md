---
phase: 04-core-inference-architecture
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - lib/core/db/app_database.dart
  - lib/core/db/app_database.g.dart
  - lib/features/inference/domain/inference_message.dart
  - lib/features/inference/domain/prompt_builder.dart
autonomous: true
requirements:
  - MODL-05
  - CHAT-04

must_haves:
  truths:
    - "Drift schema defines ChatSessions and ChatMessages tables with migration from version 1 to 2"
    - "Inference message protocol defines all command and response types needed for isolate communication"
    - "PromptBuilder produces correct Aya chat template format with system prompts for both translation and chat modes"
  artifacts:
    - path: "lib/core/db/app_database.dart"
      provides: "ChatSessions and ChatMessages table definitions with schema version 2 and onUpgrade migration"
      contains: "class ChatSessions extends Table"
    - path: "lib/features/inference/domain/inference_message.dart"
      provides: "Sealed InferenceCommand and InferenceResponse types for isolate IPC"
      contains: "sealed class InferenceCommand"
    - path: "lib/features/inference/domain/prompt_builder.dart"
      provides: "Aya chat template construction with translation and chat system prompts"
      contains: "class PromptBuilder"
  key_links:
    - from: "lib/core/db/app_database.dart"
      to: "ChatMessages.sessionId"
      via: "foreign key reference to ChatSessions.id"
      pattern: "references\\(ChatSessions"
---

<objective>
Build the three foundational layers that all subsequent Phase 4 plans depend on: Drift DB schema for chat persistence, inference message protocol for isolate communication, and PromptBuilder for Aya chat template formatting.

Purpose: These are the data contracts — every other plan in Phase 4 imports from these files. Getting them right first means no rework.
Output: Three new/modified source files plus regenerated Drift codegen.
</objective>

<execution_context>
@~/.claude/mowism/workflows/execute-plan.md
@~/.claude/mowism/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-core-inference-architecture/04-RESEARCH.md
@lib/core/db/app_database.dart
@lib/core/db/app_database.g.dart
@integration_test/helpers/model_loader.dart
</context>

<tasks>

<task type="auto">
  <name>Task 1: Drift schema — ChatSessions and ChatMessages tables with migration</name>
  <files>lib/core/db/app_database.dart, lib/core/db/app_database.g.dart</files>
  <action>
Extend the existing Phase 3 Drift database stub to add two tables and bump the schema version.

**ChatSessions table:**
- `id`: `IntColumn`, autoIncrement (primary key)
- `title`: `TextColumn`, nullable (null = auto-derived from first message content by ChatNotifier later)
- `mode`: `TextColumn` (values: 'chat' or 'translation')
- `createdAt`: `DateTimeColumn`
- `updatedAt`: `DateTimeColumn`

**ChatMessages table:**
- `id`: `IntColumn`, autoIncrement (primary key)
- `sessionId`: `IntColumn`, references ChatSessions.id (foreign key)
- `role`: `TextColumn` (values: 'user' or 'assistant')
- `content`: `TextColumn`
- `isTruncated`: `BoolColumn`, withDefault Constant(false) — marks messages stopped by user
- `createdAt`: `DateTimeColumn`

**Schema update:**
- Change `schemaVersion` from `1` to `2`
- Add `@DriftDatabase(tables: [ChatSessions, ChatMessages])` to the class annotation
- Add `MigrationStrategy` with:
  - `onCreate: (m) async => await m.createAll()`
  - `onUpgrade: (m, from, to) async { if (from == 1) { await m.createTable(chatSessions); await m.createTable(chatMessages); } }`
  - `beforeOpen: (details) async { await customStatement('PRAGMA journal_mode=WAL'); await customStatement('PRAGMA foreign_keys=ON'); }` — WAL mode enables concurrent reads during inference, foreign_keys enforces referential integrity

**Important:** The table classes `ChatSessions` and `ChatMessages` must be defined ABOVE the `AppDatabase` class in the same file (Drift convention — tables are top-level classes in the database file or separate files imported into it).

After modifying the file, run code generation:
```
/home/max/Android/flutter/bin/flutter pub run build_runner build --delete-conflicting-outputs
```

This regenerates `app_database.g.dart` with the new table companions, data classes, and manager extensions.
  </action>
  <verify>
Run `dart analyze lib/core/db/app_database.dart` and confirm zero errors. Verify `app_database.g.dart` contains generated `ChatSession` and `ChatMessage` data classes (the Drift-generated row types). Verify `schemaVersion` returns `2`.
  </verify>
  <done>
app_database.dart defines ChatSessions and ChatMessages tables with foreign key from messages to sessions. Schema version is 2 with working onUpgrade migration from version 1. Generated code compiles without errors.
  </done>
</task>

<task type="auto">
  <name>Task 2: Inference message protocol — sealed command and response types</name>
  <files>lib/features/inference/domain/inference_message.dart</files>
  <action>
Create the inference feature directory structure: `lib/features/inference/domain/`.

Create `inference_message.dart` with two sealed class hierarchies for isolate IPC. All fields must be primitive types only (int, String, bool) — Dart isolate SendPort constraint.

**Commands (main isolate → worker isolate):**

```dart
sealed class InferenceCommand {}

/// Load the GGUF model at the given path.
final class LoadModelCommand extends InferenceCommand {
  final String modelPath;
  final int nCtx;      // context window size (2048 for production)
  final int nBatch;    // batch size (256 default)
  const LoadModelCommand({required this.modelPath, this.nCtx = 2048, this.nBatch = 256});
}

/// Generate a response for the given prompt.
final class GenerateCommand extends InferenceCommand {
  final int requestId;
  final String prompt;
  final int nPredict;  // 128 for translation, 512 for chat
  const GenerateCommand({required this.requestId, required this.prompt, required this.nPredict});
}

/// Stop the current generation. Cooperative — checked between tokens.
final class StopCommand extends InferenceCommand {
  final int requestId;
  const StopCommand({required this.requestId});
}

/// Clear the KV cache. Used when starting a new session.
final class ClearContextCommand extends InferenceCommand {
  const ClearContextCommand();
}

/// Gracefully shut down the isolate and dispose of the Llama instance.
final class ShutdownCommand extends InferenceCommand {
  const ShutdownCommand();
}
```

**Responses (worker isolate → main isolate):**

```dart
sealed class InferenceResponse {}

/// Model loaded successfully — ready for generation.
final class ModelReadyResponse extends InferenceResponse {
  const ModelReadyResponse();
}

/// A single generated token.
final class TokenResponse extends InferenceResponse {
  final int requestId;
  final String token;
  const TokenResponse({required this.requestId, required this.token});
}

/// Generation complete (natural end or user stop).
final class DoneResponse extends InferenceResponse {
  final int requestId;
  final bool stopped; // true = user stopped, false = natural completion
  const DoneResponse({required this.requestId, required this.stopped});
}

/// An error occurred in the worker isolate.
final class ErrorResponse extends InferenceResponse {
  final int requestId; // -1 for load errors
  final String message;
  const ErrorResponse({required this.requestId, required this.message});
}
```

Use `@immutable` annotation from `package:flutter/foundation.dart` on both sealed base classes.
  </action>
  <verify>
Run `dart analyze lib/features/inference/domain/inference_message.dart` — zero errors. Verify all classes have only primitive fields (int, String, bool).
  </verify>
  <done>
inference_message.dart defines sealed InferenceCommand (5 variants: LoadModel, Generate, Stop, ClearContext, Shutdown) and sealed InferenceResponse (4 variants: ModelReady, Token, Done, Error). All fields are SendPort-safe primitives.
  </done>
</task>

<task type="auto">
  <name>Task 3: PromptBuilder — Aya chat template with system prompts</name>
  <files>lib/features/inference/domain/prompt_builder.dart</files>
  <action>
Create `prompt_builder.dart` in `lib/features/inference/domain/`.

Implement the `PromptBuilder` class with static methods and constants. The Aya chat template format is confirmed from Phase 1 spike (CLAUDE.md):

```
<|START_OF_TURN_TOKEN|><|USER_TOKEN|>{message}<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>
```

**Constants:**
- `_startOfTurn = '<|START_OF_TURN_TOKEN|>'`
- `_userToken = '<|USER_TOKEN|>'`
- `_chatbotToken = '<|CHATBOT_TOKEN|>'`
- `_endOfTurn = '<|END_OF_TURN_TOKEN|>'`

**System prompts (per locked decisions from CONTEXT.md):**

Translation system prompt:
```
You are a translator. Translate the given text and reply with only the translation. Add a brief note about formality or context when relevant.
```

Chat system prompt:
```
You are a translator and language assistant. Help people translate text and understand languages. If asked about other topics, mention that translation is your strength.
```

Both prompts must be short and directive — the 3.35B model ignores complex instructions (per locked decision).

**Static methods:**

1. `buildInitialPrompt({required String systemPrompt, required String userMessage})` — Returns the full prompt for the first message in a session. System prompt is prepended to the user message inside the user turn tokens. Ends with chatbot turn start (model generates from here).
   Format: `{_startOfTurn}{_userToken}{systemPrompt}\n\n{userMessage}{_endOfTurn}{_startOfTurn}{_chatbotToken}`

2. `buildFollowUpPrompt(String userMessage)` — Returns the incremental prompt for subsequent turns. Does NOT include system prompt or prior history (KV cache preserves it via `setPrompt()` appending).
   Format: `{_startOfTurn}{_userToken}{userMessage}{_endOfTurn}{_startOfTurn}{_chatbotToken}`

3. `buildTranslationPrompt({required String text, required String targetLanguage})` — Convenience wrapper that calls `buildInitialPrompt` with the translation system prompt and a formatted user message like `Translate to {targetLanguage}: {text}`. Used for the first translation in a session.

4. `buildFollowUpTranslationPrompt({required String text, required String targetLanguage})` — Same as above but calls `buildFollowUpPrompt` for subsequent translations in the same session.

5. `estimateTokenCount(String text)` — Rough estimation: ~4 chars per token for Latin scripts, ~2 chars per token for CJK. Returns the higher estimate (text.length / 2) rounded up. Used for context-full detection.

Do NOT include a `wrapAssistantResponse` method — the KV cache handles this implicitly via `setPrompt()` appending. We never reconstruct full history from strings.
  </action>
  <verify>
Run `dart analyze lib/features/inference/domain/prompt_builder.dart` — zero errors. Manually inspect that `buildInitialPrompt` output matches the Aya template format from CLAUDE.md.
  </verify>
  <done>
PromptBuilder provides static methods for Aya chat template construction. Translation and chat system prompts match locked decisions. `estimateTokenCount` provides rough token estimation for context-full detection. No history reconstruction — relies on KV cache appending.
  </done>
</task>

</tasks>

<verification>
After all three tasks:
1. `dart analyze lib/core/db/ lib/features/inference/domain/` — zero errors across all modified/created files
2. `app_database.g.dart` exists and contains `ChatSessionData` and `ChatMessageData` generated classes
3. `inference_message.dart` sealed classes cover all 5 commands and 4 responses
4. `prompt_builder.dart` `buildInitialPrompt` output starts with `<|START_OF_TURN_TOKEN|><|USER_TOKEN|>` and ends with `<|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>`
</verification>

<success_criteria>
- Drift schema compiles and generates correct codegen for ChatSessions and ChatMessages
- Inference message types are SendPort-safe (all primitive fields)
- PromptBuilder produces Aya-format prompts matching the template in CLAUDE.md
- All files pass `dart analyze` with zero errors
</success_criteria>

<output>
After completion, create `.planning/phases/04-core-inference-architecture/04-01-SUMMARY.md`
</output>
